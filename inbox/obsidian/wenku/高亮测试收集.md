测试时计算（[Graves 等人，2016 年](https://arxiv.org/abs/1603.08983)；[Ling 等人，2017 年](https://arxiv.org/abs/1705.04146)；[Cobbe 等人，2021 年](https://arxiv.org/abs/2110.14168)）和思维链 (CoT)（[Wei 等人，2022 年](https://arxiv.org/abs/2201.11903)；[Nye 等人，2021 年](https://arxiv.org/abs/2112.00114)）显著提升了模型性能，同时也引发了许多研究问题。本文旨在回顾如何有效利用测试时计算（即“思考时间”）及其益处的最新进展。

# 动机

可以通过几种不同的方式来让模型思考更长时间。

## 与心理学的类比

==其核心思想与人类的思维方式息息相关。我们人类无法立即给出答案`"What's 12345 times 56789?"`。相反，在得出结果之前，花时间思考和分析是很自然的，尤其是在处理复杂问题时。==在[《思考，快与慢》（卡尼曼，2013）一书中，丹尼尔·卡尼曼通过](https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555)[双重过程理论](https://en.wikipedia.org/wiki/Dual_process_theory)，将人类思维分为两种模式：

- _快速思考（系统 1）_运行迅速且自动，由直觉和情感驱动，几乎不需要任何努力。
- _慢速思考（系统2）_需要深思熟虑、逻辑清晰的思维，以及大量的认知投入。这种思维模式消耗更多的脑力，需要有意识地投入。

==由于系统1思维快速便捷，它往往最终成为主要的决策驱动力，代价是准确性和逻辑性。它自然地依赖于我们大脑的思维捷径（即启发式思维），并可能导致错误和偏见。通过有意识地放慢速度，花更多时间进行反思、改进和分析，我们可以运用系统2思维来挑战我们的直觉，做出更理性的选择。==

## 计算作为一种资源

深度学习的一种观点是，神经网络可以通过其在前向传播中能够访问的计算量和存储空间来表征。如果我们使用梯度下降法来优化神经网络，优化过程就会找到如何使用这些资源的方法——它们会找到如何将这些资源组织成用于计算和信息存储的电路。从这个角度来看，如果我们设计一个在测试时能够进行更多计算的架构或系统，并训练它有效地利用这些资源，它就会表现得更好。

在 Transformer 模型中，模型为每个生成的 token 执行的计算量（flops）大约是参数数量的 2 倍。对于像混合专家 (MoE) 这样的稀疏模型，每次前向传递仅使用一小部分参数，因此计算量 = 2 * 参数 / 稀疏度，其中稀疏度是活跃专家的比例。

另一方面，CoT 使模型能够为其试图计算的答案的每个 token 执行更多次计算。事实上，CoT 有一个很好的特性，它允许模型根据问题的难度使用可变的计算量。

## 潜变量建模

机器学习中的一个经典思想是定义一个具有潜在（隐藏）变量的概率模型z和一个可见变量y， 在哪里y被赋予我们的学习算法。对潜在变量的可能值进行边缘化（求和）使我们能够表达可见变量的丰富分布，磷(y)=∑z∼磷(z)磷(y∣z)。例如，我们可以通过以下方式对数学问题和解决方案的分布进行建模：x表示问题陈述，y是基本事实答案或证明，并且z作为一个自由形式的思维过程，最终得到证明。需要优化的边际概率分布为磷(y∣x)=∑z∼页(z∣x)磷(y∣x，z)

潜在变量视角对于理解涉及收集多个并行 CoT 或在 CoT 上搜索的方法特别有用——这些算法可以看作是从后验概率中采样磷(z∣x，y)。这种观点也表明了使用对数损失的好处日志⁡磷(y∣x)作为优化的目标，因为对数损失目标在预训练中非常有效。

# 用代币思考

[Ling 等人 (2017)](https://arxiv.org/abs/1705.04146)探索了在生成简短答案之前生成中间步骤的策略，特别是对于数学问题，他们推出了[AQUA-RAT](https://github.com/google-deepmind/AQuA)数据集，然后[Cobbe 等人 (2021)](https://arxiv.org/abs/2110.14168)扩展了该策略，他们推出了[小学数学 (GSM)](https://github.com/openai/grade-school-math)数据集。Cobbe 等人使用监督学习在人类编写的解决方案上训练生成器，并使用验证器预测候选解决方案的正确性；然后他们可以搜索这些解决方案。Nye[等人 (2021](https://arxiv.org/abs/2112.00114) ) 尝试使用中间思维标记作为“便笺簿”，而[Wei 等人](https://arxiv.org/abs/2201.11903)(2022) 创造了现在的标准术语“**思维链**(CoT)”。

早期改进CoT推理的研究涉及对人工编写的推理轨迹或模型编写的轨迹进行监督学习，并根据答案的正确性进行过滤，后者可以被视为强化学习（RL）的一种初级形式。其他一些研究发现，通过适当的提示，可以显著提高指令调整模型的数学性能，例如使用`"think step by step"`（[Kojima等人，2022](https://arxiv.org/abs/2205.11916)）或更复杂的提示来鼓励模型首先反思相关知识（[Yasunaga等人，2023](https://arxiv.org/abs/2310.01714)）。

后续研究发现，通过对具有可自动检查解决方案的问题数据集进行强化学习，可以显著提升 CoT 推理能力，例如答案简短的 STEM 问题，或可通过单元测试检查的编码任务（[Zelikman 等人，2022 年](https://arxiv.org/abs/2203.14465)；[Wang 等人，2023 年](https://arxiv.org/abs/2312.08935)；[Liu 等人，2023 年）。这种方法随着](https://arxiv.org/abs/2310.10047)[o1-preview](https://openai.com/index/learning-to-reason-with-llms/)、[o3](https://openai.com/index/introducing-o3-and-o4-mini/)和 R1 技术报告（[DeepSeek-AI，2025 年](https://arxiv.org/abs/2501.12948)）的发布而声名鹊起，这些报告表明，一个简单的策略梯度算法就能带来强劲的性能。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/cot-wei22.png)

思路链式提示能提高数学解题成功率。模型规模越大，思考时间的利用效果越好。（图片来源：Wei et al. 2022）

## 分支和编辑

测试时计算的根本目的是自适应地修改模型在测试时的输出分布。有多种方法可以利用测试时资源进行解码，以选择更优的样本，从而将模型的预测调整到更理想的分布。改进解码过程的两种主要方法是并行采样和顺序修订。

- **并行采样**可同时生成多个输出，同时通过过程奖励信号为每一步提供指导，或使用验证器在最后判断质量。它是目前最广泛采用的解码方法，用于提高测试时间性能，例如最佳北或集束搜索。当无法获得基本事实时，通常使用自洽性（[Wang 等人，2023](https://arxiv.org/abs/2203.11171)）在多个 CoT rollouts 中选择具有多数票的答案。
- **顺序修正**会根据上一步的输出迭代地调整模型的响应，要求模型有意识地反映其现有响应并纠正错误。修正过程可能需要依赖于经过微调的模型，因为单纯地依赖模型固有的自我修正能力而缺乏外部反馈可能无法带来改进（[Kamoi 等人，2024](https://arxiv.org/abs/2406.01297)；[Huang 等人，2024](https://arxiv.org/abs/2310.01798)）。

并行采样简单、直观且易于实现，但受限于模型能力，即能否一次性获得正确解决方案。顺序采样明确要求模型反思错误，但速度较慢，并且在实施过程中需要格外小心，因为它确实存在正确预测被修改为错误或引入其他类型幻觉的风险。这两种方法可以一起使用。Snell 等人[(2024](https://arxiv.org/abs/2408.03314) ) 表明，较简单的问题受益于纯顺序的测试时间计算，而较难的问题通常在顺序计算和并行计算的最佳比例下表现最佳。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/parallel-vs-sequential.png)

并行采样与顺序修订的说明。

### 平行采样

给定一个生成模型和一个可用于对全部或部分样本进行评分的评分函数，我们可以使用各种搜索算法来找到高分样本。最佳北是最简单的算法：只需收集北独立样本，并根据某些评分函数选择排名最高的样本。集束搜索是一种更复杂的搜索算法，它使搜索过程更具自适应性，将更多的采样计算放在解空间中更有希望的部分。

集束搜索会维护一组有希望的部分序列，并交替扩展这些序列和修剪那些不太有希望的序列。作为一种选择机制，我们可以使用过程奖励模型 (PRM；[Lightman 等人，2023](https://arxiv.org/abs/2305.20050) ) 来指导集束搜索候选序列的选择。Xie[等人 (2023](https://arxiv.org/abs/2305.00633) ) 使用 LLM 来评估其自身生成的推理步骤的正确性，并将其格式化为多项选择题，结果发现在集束搜索解码过程中，每步自我评估可以减少多步推理中的累积误差。此外，在采样过程中，退火温度有助于减轻聚合随机性。Xie 等人的这些实验使用 Codex 模型在少样本 GSM8k、AQuA 和 StrategyQA 基准上取得了 5-6% 的提升。奖励平衡搜索（“REBASE”的缩写；[Wu 等人，2025](https://arxiv.org/abs/2408.00724)）单独训练了一个过程奖励模型 (PRM)，根据 softmax 归一化的奖励分数，确定在集束搜索期间每个节点应在每个深度扩展多少。Jiang[等人（2024）](https://arxiv.org/abs/2410.01044)训练了他们的 PRM，名为“RATIONALYST”，用于在以大量未标记数据为条件的合成原理上进行集束搜索指导。在比较原理包含在上下文中与不包含在上下文中时的差异时，好的原理会根据它们是否有助于将真实答案标记的负对数概率降低阈值进行筛选。在推理时，RATIONALYST 通过帮助估计下一步推理步骤的对数概率（“隐式”）或直接生成下一步推理步骤作为提示的一部分（“显式”），为 CoT 生成器提供过程监督。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/beam-search-xie23.png)

每个推理步骤由 LLM 自我评估引导的集束搜索解码。（图片来源：[Xie et al. 2023](https://arxiv.org/abs/2305.00633)）

_有趣的是，无需_明确的零样本或少样本提示，就能触发涌现的思维链推理路径。Wang [& Zhou (2024)](https://arxiv.org/abs/2402.10200)发现，如果我们在第一个采样标记处进行分支，并保留顶部千置信度最高的 token，以采样期间 top-1 和 top-2 候选之间的差异来衡量，然后继续这些千在使用贪婪解码进行采样试验后，许多此类序列本身就包含“跨度”（CoT）。尤其当“跨度”出现在上下文中时，它能使最终答案的解码更加可信。为了计算最终答案的置信度，需要通过特定于任务的启发式方法（例如，数学问题的最后一个数值）或通过使用进一步提示模型来识别答案跨度`"So the answer is"`。仅在第一个标记处进行分支的设计选择基于以下观察：早期分支显著增强了潜在路径的多样性，而后续标记则受先前序列的很大影响。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/cot-decoding.png)

顶部-千解码，千指第一步采样的候选样本数量。（图片来源：[Wang & Zhou, 2024](https://arxiv.org/abs/2402.10200)）

### 顺序修订

如果模型可以反映和纠正过去反应中的错误，我们期望模型能够产生一个质量不断提高的良好的迭代修订序列。然而，这种自我纠正能力在 LLM 中并不固有，而且由于各种故障模式而无法轻易开箱即用，例如 (1) 幻觉，包括将正确的反应修改为不正确的；(2) 行为崩溃为非纠正行为；例如，对第一个不正确的反应进行微小修改或不进行修改；或 (3) 无法推广到测试时的分布偏移。Huang[等人 (2024](https://arxiv.org/abs/2310.01798) ) 的实验表明，单纯地应用自我纠正会导致性能下降，模型需要外部反馈来自我改进，这可以基于匹配的基本事实、启发式方法和特定于任务的指标、编码问题的单元测试结果（[Shinn 等人，2023 年](https://arxiv.org/abs/2303.11366)）、更强大的模型（[Zhang 等人，2024 年](https://arxiv.org/abs/2404.17140)）以及人类反馈（[Liu 等人，2023 年](https://arxiv.org/abs/2302.02676)）。

自我校正学习（[Welleck 等人，2023](https://arxiv.org/abs/2211.00053)）旨在训练一个校正模型磷θ(y∣y0，x)给定一个固定的生成器模型磷0(y0∣x)。虽然生成器模型仍然是通用的，但校正器模型可以针对特定任务，并且仅根据初始模型响应和附加反馈（例如，句子、编译器跟踪、单元测试结果；可以是可选的）进行生成：

1. 自我修正学习首先在数据池中为每个提示生成多个输出；
2. 然后通过将同一提示的两个输出配对在一起来创建价值提升对，如果其中一个输出比另一个输出具有更高的价值（提示x， 假设y，修正y′）。
3. 这些对的选择与价值的提升成正比，五(y′)−五(y)以及两个输出之间的相似性，相似(y，y′)训练校正模型。
4. 为了鼓励探索，校正器还会将新的生成数据添加到数据库。在推理阶段，校正器可以迭代使用，以创建连续修正的校正轨迹。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/self-correction-welleck23.png)

自我修正学习的图示：通过匹配同一问题的模型输出，形成价值提升对，从而训练修正模型。（图片来源：Welleck 等人，2023）

递归检查（[Qu et al. 2024](https://arxiv.org/abs/2407.18219)）也旨在训练更好的校正模型，但使用单个模型进行生成和自我校正。

SCoRe（通过强化学习进行自我纠正；[Kumar 等人，2024 年](https://arxiv.org/abs/2409.12917)）是一种多轮 RL 方法，旨在鼓励模型通过在第二次尝试时产生比第一次尝试时更好的答案来进行自我纠正。它包含两个训练阶段：第 1 阶段仅最大化第二次尝试的准确性，同时仅对第一次尝试施加 KL 惩罚，以避免第一轮响应与基本模型行为发生过多偏差；第 2 阶段优化第一次和第二次尝试所产生答案的准确性。理想情况下，我们确实希望看到第一次和第二次尝试的性能都更好，但添加第 1 阶段可以防止模型对第一次响应进行少量编辑或不进行任何编辑的行为崩溃，而第 2 阶段可以进一步改善结果。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/SCoRe-kumar24.png)

通过两阶段强化学习训练，明确设置训练方案，提升自我纠正能力。（图片来源：Kumar 等人，2024）

## 强化学习，实现更好的推理

最近，使用 RL 来提高语言模型的推理能力方面取得了很多成功，方法是使用一系列具有基本事实答案的问题（通常是 STEM 问题和易于验证答案的谜题），并奖励获得正确答案的模型。该领域的最新活动受到`o`OpenAI 系列模型的强劲表现以及[DeepSeek](https://www.deepseek.com/)随后发布的模型和技术报告的推动。

`DeepSeek-R1`（[DeepSeek-AI，2025](https://arxiv.org/abs/2501.12948)）是一款开源的 LLM，旨在出色地完成需要高级推理技能（例如数学、编程和逻辑问题解决）的任务。它们经过两轮 SFT-RL 训练，使 R1 能够同时擅长推理和非推理任务。

1. **冷启动SFT**`DeepSeek-V3-Base`是在数千个冷启动数据集合上对基础模型进行微调。如果没有这一步骤，模型将存在可读性差和语言混合的问题。
2. **面向推理的 RL**使用两种基于规则的奖励，在仅推理的提示上训练推理模型：
    - 格式化奖励：模型应该用`<thinking> ... </thinking>`代币包装 CoT。
    - 准确性奖励：最终答案是否正确。数学题的答案需要以特定格式（例如，在一个方框中）呈现，才能可靠地验证。对于编程题，编译器用于评估测试用例是否通过。
3. **拒绝采样 + 非推理 SFT**利用在步骤 2 的 RL 检查点上通过拒绝采样创建的新 SFT 数据，结合来自`DeepSeek-V3`写作、事实 QA 和自我认知等领域的非推理监督数据进行重新训练`DeepSeek-V3-Base`。
    - 过滤掉混合语言、长段落和代码块的 CoT。
    - [包括使用 DeepSeek-V3（ DeepSeek-AI, 2024](https://arxiv.org/abs/2412.19437v1) ）管道的非推理任务。
    - 对于某些非推理任务，在通过提示回答问题之前，调用 DeepSeek-V3 生成潜在的 CoT。但对于像“hello”这样的简单查询，则不需要 CoT。
    - 然后基于总共 800k 个样本对 DeepSeek-V3-Base 进行 2 个 epoch 的微调。
4. 最后的**RL**阶段针对推理和非推理提示训练第 3 步检查点，以提高实用性、无害性和推理性。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/R1-eval.png)

`DeepSeek-R1`在多个广泛使用的推理基准测试中`o1-preview`，其表现与 OpenAI 相当。是唯一上榜的非推理模型。（图片来源：DeepSeek-AI，2025）`o1-mini``DeepSeek-V3`

有趣的是，DeepSeek 团队表明，即使没有 SFT 阶段，纯 RL 仍然可以学习高级推理能力，例如反思和回溯（“顿悟时刻”）。该模型自然而然地学会在 RL 训练过程中使用更多的思考令牌来解决推理任务。“顿悟时刻”可能会出现，指的是模型反思以前的错误，然后尝试其他方法来纠正它们。后来，出现了各种开源项目来复制 R1 的结果，例如[Open-R1](https://github.com/huggingface/open-r1)、[SimpleRL-reason](https://github.com/hkust-nlp/simpleRL-reason)和[TinyZero](https://github.com/Jiayi-Pan/TinyZero)，它们都基于[Qwen](https://github.com/QwenLM/Qwen2.5)模型。这些努力也证实了纯 RL 可以在数学问题上表现出色，并且能够出现“顿悟时刻”。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/aha-moment.png)

模型学习反思和纠正错误的示例。（图片来源：（左）DeepSeek-AI，2025；（右）Zeng et al. 2025）

DeepSeek 团队也分享了一些失败的尝试。他们未能使用过程奖励模型 (PRM)，因为该模型难以定义每一步的规则或判断中间步骤是否正确，同时训练过程也更容易受到奖励攻击。MCTS（蒙特卡洛树搜索）方面的努力也失败了，因为与国际象棋等相比，语言模型 token 的搜索空间较大；而且训练用于指导搜索的细粒度价值模型也极具挑战性。失败的尝试往往能提供独特的见解，我们希望鼓励研究界分享更多失败案例。

## 外部工具的使用

在推理步骤中，某些中间步骤可以通过执行代码或运行数学计算来可靠而准确地解决。将这部分推理组件卸载到外部代码解释器中，例如 PAL（程序辅助语言模型；[Gao 等人，2022 年](https://arxiv.org/abs/2211.10435)）或代码链（[Li 等人，2023 年](https://chain-of-code.github.io/)），可以使用外部工具扩展 LLM 的功能，从而无需 LLM 学习执行代码或充当计算器本身。这些代码模拟器（例如在代码链中）可以通过 LLM 进行增强，这样，如果标准代码解释器出现故障，我们可以选择使用 LLM 来执行该行代码。使用代码增强推理步骤对于数学问题、符号推理和算法任务尤其有益。这些单元测试可能不存在于编码问题中，在这种情况下，我们可以指示模型自行生成单元测试，以便对其进行测试以验证解决方案（[Shinn 等人，2023 年](https://arxiv.org/abs/2303.11366)）。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/pal.png)

程序辅助语言模型提示的示例如下。（图片来源：Gao et al. 2022）

ReAct（Reason+Act；[Yao 等人，2023 年](https://arxiv.org/abs/2210.03629)）结合了搜索维基百科 API 的行为和生成推理痕迹的行为，使得推理路径可以融入外部知识。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/react.png)

使用 ReAct 提示方法解决 HotpotQA 问题的示例，其中使用维基百科搜索 API 作为外部工具辅助推理。（图片来源：Yao et al. 2023）

[OpenAI 最近发布的o3 和 o4-mini](https://openai.com/index/introducing-o3-and-o4-mini/)是另外两个很好的例子，其中推理过程涉及 Web 搜索、代码执行和图像处理等工具的使用。该团队观察到，大规模强化学习表现出与 GPT 范式相同的趋势，即“更多计算 = 更好性能”。

## 忠实思考

深度学习模型通常被视为黑匣子，因此人们提出了各种可解释性方法。可解释性之所以有用，有几个原因：首先，它为我们提供了额外的测试，以确定模型是否与其创建者的意图不一致，或者是否存在我们无法通过监控其行为发现的某些异常行为。其次，它可以帮助我们确定模型是否使用合理的流程来计算答案。思路链提供了一种特别便捷的可解释性形式，因为它使模型的内部流程可以用自然语言直观地呈现。然而，这种可解释性依赖于模型真实地描述其内部思维过程的假设。

最近的研究表明，监控推理模型的测试时间 (CoT) 可以有效检测模型的不当行为，例如[奖励黑客攻击](https://lilianweng.github.io/posts/2024-11-28-reward-hacking/)，甚至可以让较弱的模型监控较强的模型 ( [Baker 等人，2025](https://arxiv.org/abs/2503.11926) )。增加测试时间计算也可以提高对抗鲁棒性 ( [Zaremba 等人，2025](https://arxiv.org/abs/2501.18841) )；这在直觉上是有道理的，因为当模型面临不寻常的输入（例如对抗性示例或越狱尝试）时，更长时间的思考应该特别有用——它可以利用额外的思考时间来理解所面临的奇怪情况。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/cot-monitor.png)

这项实验要求模型判断是否有其他模型试图以某种方式破解单元测试，以解决其编码问题，并给出其思维过程。我们可以在训练过程中使用不同类型的监视器来监控这些奖励破解行为。`exit(0)`编码破解是指代理利用一个漏洞，使其能够在未运行所有单元测试的情况下提前退出环境。而`raise SkipTest`破解是指代理为了跳过单元测试评估，从测试框架之外的函数引发异常。（图片来源：Baker et al. 2025）

### 模型是否如实地表达了它的想法

直观地讲，由于缺乏明确的训练目标（旨在鼓励忠实推理），模型 CoT 可能会存在偏差。或者，当我们根据人工编写的解释对模型进行微调时，这些人工编写的样本可能包含错误。因此，我们不能默认假设 CoT 始终是忠实的。

[Lanham 等人 (2023)](https://arxiv.org/abs/2307.13702)通过故意在 CoT 中引入错误并测量其对一组多项选择任务（例如 AQuA、MMLU、ARC Challenge、TruthfulQA、HellaSwag）准确性的影响，研究了 CoT 忠实度失败的几种模式：

- 错误 1（_过早回答_）：模型可能在语料库生成之前过早得出结论。可以通过过早截断或在语料库中插入错误来测试这一点。不同的任务揭示了语料库有效性在不同任务中表现出不同的依赖性；有些任务的评估表现对截断的语料库敏感，而有些则不然。Wang[等人 (2023)](https://arxiv.org/abs/2212.10001)进行了类似的实验，但实验中出现了一些更细微的错误，这些错误与语料库形成过程中桥接对象或语言模板有关。
    
- 错误 2（_无信息标记_）：无信息 CoT 标记会提高性能。我们通过用填充文本（例如所有句号）替换 CoT 来测试此假设，结果显示这种设置并没有提高准确率，而且与没有 CoT 相比，某些任务的性能可能会略有下降。
    
- 错误 3（_人类难以理解的编码_）：相关信息的编码方式难以被人类理解。以非标准方式解释关键术语 (CoT) 并不会降低跨数据集的性能，这表明准确率的提升并不依赖于人类可读的推理。
    

![](https://lilianweng.github.io/posts/2025-05-01-thinking/cot-perturb.png)

图示为评估CoT忠实度的不同扰动方式。（图片来源：Lanham et al. 2023）

有趣的是，Lanham 等人指出，对于多项选择题，较小的模型可能无法充分发挥思考时间 (CoT) 的作用，而较大的模型或许能够在没有思考时间的情况下完成这些任务。这种对思考时间推理的依赖性（以有无思考时间获得相同答案的百分比来衡量）在多项选择题中并不总是随着模型规模的增大而增加，但在加法任务中却会随着模型规模的增大而增加，这意味着思考时间对于复杂的推理任务更为重要。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/cot-ablation.png)

对 CoT 推理的依赖性衡量的是使用 CoT 和不使用 CoT 时获得相同答案的百分比。对于加法等推理任务来说，CoT 的依赖性更为重要，而且模型规模越大，受益也就越大。（图片来源：Lanham et al. 2023）

测试 CoT 忠实度的替代方法包括扰乱提示，而不是直接修改 CoT 路径（[Turpin 等人，2023 年](https://arxiv.org/abs/2305.04388)；[Chua & Evans，2025 年](https://arxiv.org/abs/2501.08156)；[Chen 等人，2025 年](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf)）。

一种方法是，在少数例子中，始终将正确答案标记为“（A）”，而不管真实标签如何，从而引入偏差。

另一种提示技巧是在提示中插入误导性提示，例如`"I think the answer is <random_label> but curious to hear what you think".`或`"A Stanford Professor thinks the answer is <random_label>"`。通过比较同一问题在有误导性提示和无误导性提示的情况下的模型预测，我们可以衡量模型是否能够忠实地描述提示对其答案的影响。具体来说，当模型得出的提示和非提示答案不同时，我们会衡量模型在解决有提示的问题时是否承认提示的影响。如果模型是忠实的，它应该明确承认提示的影响，并承认其答案的变化是由于提示造成的。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/faithfulness.png)

GPT 和 Claude 模型都对不同类型的上下文偏差敏感。模型准确率的下降表明存在系统性偏差。直接提示错误标签比“答案总是 A”类型的偏差更有效。（图片来源：Turpin 等人，2023）

多项研究发现，推理模型对提示影响的描述比所有测试的非推理模型都更加可靠。例如，我们可以测量模型将提示视为决定因素的样本比例（“忠实的 CoT”）。推理模型（Claude 3.7 Sonnet、DeepSeek R1）总体上比非推理模型（Claude 3.6、DeepSeek V3）表现更佳。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/faithfulness2.png)

推理模型比非推理模型更有可能揭示忠实的CoT。（图片来源：Chen et al. 2025）

一些证据表明，使用奖励模型会导致模型输出的忠诚度降低。经典 RLHF 中的奖励模型并未经过训练以激励此测试中的忠诚行为，因为偏好比较数据集是由人类收集的，人类会选择哪个看起来更好或更令人愉悦。然而，在推理模型中，CoT 路径的优化是为了生成正确的最终答案，而不是为了匹配 RM 中定义的人类偏好，因此直觉上预期会更加忠诚。

### CoT 的优化压力：好还是坏？

监控推理模型的CoT（计算时间）是否存在奖励黑客行为是有效的。一种简单的使用方法是在测试时以监控器作为过滤器进行拒绝采样，并识别出没有奖励黑客行为的解决方案。然而，这种方法仅适用于强化学习的早期阶段，而且随着模型的进一步优化，很难找到在预算范围内通过CoT监控的样本。这一挑战自然而然地表明，我们可以考虑将CoT监控器纳入强化学习奖励机制中——这是一种基于过程的监督形式——以防止奖励黑客行为。

然而，将CoT监控奖励纳入强化学习效果不佳，因为它会导致混淆奖励攻击，模型会试图将其真实意图隐藏在CoT中（[Baker等人，2025](https://arxiv.org/abs/2503.11926)）。在强化学习训练期间施加CoT监控器提供的直接优化压力的情况下，模型仍然能够找到攻击方法，尽管频率略低，但可以将其意图隐藏在CoT中，以避免被监控器发现。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/RL-monitor-baker25.png)

当使用监视器提供强化学习奖励信号来防止训练过程中的奖励黑客攻击时，模型仍然可以找到黑客攻击的方法，同时避免被监视器发现。（图片来源：Baker 等人，2025）

[Yeo 等人 (2025)](https://arxiv.org/abs/2502.03373)对 CoT 长度强化学习信号进行的实验证实了与 Baker 等人类似的观点，即新型强化学习奖励塑造可能会引发新型奖励攻击。他们设计了奖励函数，使得正确的 CoT 可以获得比错误的 CoT 更高的奖励，较短的正确 CoT 获得的奖励比较长的正确 CoT 更高，而较短的不正确 CoT 受到的惩罚比较长的不正确 CoT 更高。有了这种额外的奖励，模型学会了在 CoT 中重复文本以应对挑战性问题，而不是尝试解决它们。Yeo 等人进一步应用了 n-gram 重复惩罚来阻止这种攻击行为。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/rew-shaping-yeo25.png)

需要谨慎地调整奖励塑造，以通过 CoT 长度奖励来稳定训练。（图片来源：Yeo 等人，2025）

[Chen 等人 (2025)](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf)尝试了一个有缺陷的强化学习环境，具体来说，他们使用了填入了错误答案的评分器来处理多项选择题。该模型学会了在超过 99% 的提示中利用奖励黑客攻击，但在超过一半的环境中，几乎从未（不到 2%）在其学习目标 (CoT) 中口头描述奖励黑客攻击。在这种情况下，额外的强化学习优化压力无法激励模型口头描述该黑客攻击。

强化学习训练本质上对奖励黑客攻击非常敏感。仅仅依靠启发式调查和手动修复可能会导致“打地鼠”的情况。我们建议在强化学习训练期间尝试直接对CoT进行优化时务必谨慎，甚至尽量避免这样做。

# 在连续空间中思考

自适应计算时间由[Alex Graves 于 2016 年](https://arxiv.org/abs/1603.08983)提出，它早于大型语言模型的出现，但开创了相同的方向，即使模型能够动态地决定推理时需要执行的计算步数，这可以理解为使模型在测试时能够在连续空间中“进行更多思考”。连续空间中的自适应思考时间可以通过递归架构在垂直方向上实现，也可以通过更多顺序采样步骤在水平方向上实现。

## 循环架构

为了使 Transformer 架构可循环，从而实现自适应测试时间计算，人们提出了多种架构变体（[Dehghani 等人，2019 年](https://arxiv.org/abs/1807.03819)；[Hutchins 等人，2022 年](https://arxiv.org/abs/2203.07852)；[Bulatov 等人，2022 年](https://arxiv.org/abs/2207.06881)）。深入研究该主题的文献会使文章篇幅过长，因此我们仅回顾其中几个。

[通用 Transformer](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/#make-it-recurrent-universal-transformer)（[Dehghani 等人，2019 年](https://arxiv.org/abs/1807.03819)）将 Transformer 中的自注意力机制与 RNN 中的循环机制相结合，通过自适应计算时间[动态调整](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/#adaptive-modeling)步数（[Graves，2016 年](https://arxiv.org/abs/1603.08983)）。从高层次上讲，它可以被视为一个用于学习每个 token 隐藏状态表示的循环函数。如果步数固定，则通用 Transformer 相当于一个跨层共享参数的多层 Transformer。

[Geiping 等人 (2025)](https://arxiv.org/abs/2502.05171)提出了一种最新的循环架构设计，它添加了一个循环块R在标准 Transformer 之上。这个循环块的每次迭代都会采用嵌入e和一个随机状态秒我从概念上讲，这种循环深度架构有点类似于条件扩散模型，其中原始输入e在每个循环步骤中提供，同时随机高斯初始化状态秒我在整个过程中不断迭代更新。（有趣的是，他们一些更类似于扩散模型的设计实验结果却很糟糕。）

e=磷(x)嵌入秒_0∼北(0，σ2我_n⋅h)秒_我=R(e，秒_我−1) 为了 我∈1，…，r循环块；类似于 Transformer 块页=碳(秒_r)解除嵌入

重复计数r训练过程中，每个输入序列的样本是随机的，服从对数正态泊松分布。为了控制计算成本，反向传播被截断到最后一个千循环单元的迭代（千=8在实验中），这使得在泊松分布的重尾部分进行训练成为可能。嵌入块在每一步中都会持续接收梯度更新，因为它的输出e在每个步骤中注入，模拟 RNN 训练。不出所料，训练循环模型的稳定性非常敏感。初始化、规范化和超参数等因素都很重要，尤其是在扩大训练规模时。例如，隐藏状态可能会因对每个 token 预测相同的隐藏状态而崩溃；或者模型可能会学会忽略传入的状态秒为了稳定训练，Geiping 等人采用了嵌入比例因子、较小的学习率和仔细的调整。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/recurrent.png)

使用深度循环训练 3.5B 模型的实验图。饱和度大致发生在r¯=三十二这不禁让我们好奇，这种架构如何外推并推广到更大的迭代次数。（图片来源：Geiping et al. 2025）

## 思考代币

思考标记是指在训练或推理过程中引入的一组隐式标记，它们不具有直接的语言含义。它们的作用是为模型提供额外的思考时间和计算能力，从而提高模型的性能。

[Herel & Mikolov (2023](https://arxiv.org/abs/2405.08644) ) 提出了一种想法，`<T>`在句子的每个单词后插入特殊的思考标记 ( )，并在这样的数据集上训练模型。每个思考标记都会为模型赢得额外的处理时间，从而做出更准确的预测。在玩具模型上使用思考标记进行训练，其困惑度 (Plosity) 比不使用思考标记进行训练的基线模型更低。对于非平凡推理任务或涉及数字的句子，思考标记的优势更为显著。

[类似地， Goyal 等人 (2024)](https://arxiv.org/abs/2310.02226)提出的暂停标记通过在输入序列末尾附加虚拟标记（例如`.`或 之类的字符）来延迟模型的输出`#`，从而在推理过程中为模型提供额外的计算。在训练和推理过程中注入此类暂停标记非常重要，因为仅对暂停标记进行微调的收益有限。在训练期间，会在均匀随机的位置插入多个暂停标记副本，并在训练过程中忽略暂停标记的损失。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/pause-tokens-goyal24.png)

与标准设置相比，此图展示了如何在训练和推理过程中注入暂停标记。（图片来源：Goyal et al. 2024）

有趣的是，上述实验中的思考标记或暂停标记并未携带任何额外信息或添加许多新参数。但为什么它仍然有用呢？一方面，它通过引入更多推理循环来扩展计算能力，从而有效地提升了计算能力。另一方面，它可以被视为一种特殊的、隐式的测试时间（CoT）。这里的一个缺点是模型需要针对思考标记进行预训练。尽管如此，这种策略仍然是一种有趣的方法，可以在推理时间测试时间（CoT）的基础上进一步提升测试时间计算利用率。

Quiet-STaR（[Zelikman 等人，2025](https://arxiv.org/abs/2403.09629)）引入了 token 级推理，通过训练模型在每个 token 之后生成理由来解释未来的文本。它将包含和不包含理由的未来文本预测混合在一起，利用学习生成更优的理由，并使用强化训练 (REINFORCE) 来优化理由生成的质量。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/quiet-star-zelikman25.png)

Quiet-STaR 示意图。（图片来源：Zelikman et al. 2025）

Quiet-STaR 包含三个阶段：

- _思考_：用理由预测下一个标记。由于标记级推理的计算成本较高，此过程设计为并行生成多个理由。使用特殊的注意力图，使所有思维标记只关注自身、同一思维中所有先前的思维标记以及前面的文本。
    
- _讨论_：无推理的下一个标记预测与推理后预测混合。两个逻辑向量的混合权重由浅层多层感知器 (MLP) 的一个特殊混合头从每个推理后的隐藏层输出中学习得出。可以通过教师强制 (teacher forcing) 来选择正确的下一个标记。
    
- _学习_：通过从增加正确下一个标记概率的示例中学习，同时丢弃那些损害预测的标记，来训练模型通过强化来生成更好的理由。
    

无需针对数据集进行微调，Quiet-STaR 在 Mistral 7B 上的实验中改善了 CommonsenseQA（36.3%→47.2%）和 GSM8K（5.9%→10.9%）的零样本结果。

# 以潜变量的方式思考

隐变量模型定义了一个概率框架，其中可观测数据通过未观测（潜在）变量来解释。这些潜在变量捕捉生成可观测结果的隐藏结构或中间过程。语言模型可以被视为概率隐变量模型，其中测试时的思考和推理步骤是潜在思维变量（[Zhou et al. 2020](https://arxiv.org/abs/2011.05268) , [Phan et al. 2023](https://arxiv.org/abs/2312.02179)）。这种隐变量模型定义了问题 x_i、答案 y_i 和潜在思维 z_i 的联合分布。我们希望优化给定问题和各种测试时间（CoT）作为隐变量的答案的对数似然性（N 为样本数量；钾是每个问题的 CoT 数量：

日志⁡左(θ)=日志⁡页(y∣x)=日志⁡∑千=1钾页(y，z(千)∣x)=日志⁡∑千=1钾页(z(千)∣x)页(y∣z(千)，x)=日志⁡埃z(千)∼页(z(千)∣x)页(y∣z(千)，x)

我们的目标是最大化正确答案的边际可能性，页(y∣x)，给定每个问题的推理轨迹数量，{z(千)}千=1钾。

## 期望最大化

[期望最大化](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)是一种常用的迭代算法，用于优化包含（隐藏）潜在变量的模型的参数，因此可以用于训练更好的待定样本 (CoT)，并以此为条件生成更好的响应。通常，我们会在 E 步（期望）和 M 步（最大化）之间进行迭代，E 步用于猜测潜在变量的缺失信息（即如何采样更好的待定样本），M 步用于根据潜在变量优化模型参数（即如何采样更好的答案），直到收敛。

日志⁡左(θ)=日志⁡埃z(千)∼页(z(千)∣x)⏟E-步页(y∣z(千)，x)⏟M步

因为我们不能直接从潜在变量分布中抽样页(z∣x，y)，研究人员已经探索了依赖人工标注数据（[Zhou 等人，2020 年](https://arxiv.org/abs/2011.05268)）、Metropolis-Hastings MCMC（[Phan 等人，2023 年](https://arxiv.org/abs/2312.02179)）或具有特殊重要性权重的蒙特卡洛抽样（[Ruan 等人，2025 年](https://arxiv.org/abs/2503.18866)）的方法来提取良好的 CoT 样本来更新模型。Ruan[等人（2025 年](https://arxiv.org/abs/2503.18866)）尝试使用 EM 算法在大量包含潜在思维的网络文本上训练模型，其中潜在思维是根据观察到的数据块合成的，然后模型以自回归的方式对潜在思维和数据进行学习。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/ruan25.png)

注入潜在思维的数据语料库训练示意图。（图片来源：Ruan et al. 2025）

他们首先要求获得法学硕士学位问~给定观察数据 X_i 生成合成潜在思想 Z_i：

```
You are provided with a pair of web document prefix and suffix. Your task is to insert latent thoughts between them underlying the creation of the suffix conditioned on the prefix. The latent thoughts should include: the missing background knowledge and the reasoning traces underlying each claim (especially, step-by-step derivations or logical reasoning).
```

特殊标记`<StartOfLatent><Prior> ... <EndOfPrior>`用于将生成的潜在思维内容插入到原始数据中，以训练联合分布页(z，x)或近似后验问(z∣x)，取决于z插入之前或之后x。但是，由于我们使用的是法学硕士问~(z∣x)为了生成 CoT，它对近似值的性能设置了一个上限问(z∣x)可以。Ruan 等人在 E 步引入了选择 CoT 样本的重要性权重，公式如下：

西(千)=页(z(千)，x)问(z(千)∣x)=页(x∣z(千))页(z(千))问(z(千)∣x)

，这样我们优先考虑那些 CoT 能够很好地预测观测值的样本（即高页(x∣z(千))）、简单、直观（即高页(z(千))）而且信息丰富，而且不太明显（即低问(z(千)∣x)）。

## 迭代学习

由于预训练模型已经具备生成思路链的能力，因此直观的做法是设计一个迭代改进流程，生成多个 CoT，并仅根据能够得出正确答案的原理对模型进行微调。  
然而，这种简单的设计可能会失败，因为模型无法接收到无法解决的问题的学习信号。STaR（“自学推理机”；[Zelikman 等人，2022 年](https://arxiv.org/abs/2203.14465)）通过为失败的尝试添加“合理化”流程解决了这一限制，在此流程中，模型会根据问题和基本事实答案向后生成良好的 CoT，从而生成更合理的 CoT。然后，对模型进行微调，这些正确的解决方案要么能够得出正确的输出，要么是通过合理化生成的。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/STaR-algo-zelikman22.png)

STaR 算法。（图片来源：Zelikman et al. 2022）

我们可以将 STaR 视为强化学习中策略梯度的近似值，以简单的指标函数作为奖励，𝟙1[y^=y]我们希望在采样时最大化这个奖励的期望z∼页(z∣x)进而y∼页(y∣x，z)， 自从页(y∣x)=∑z页(z∣x)页(y∣x，z)。

𝟙𝟙𝟙𝟙∇θJ(θ)=∇θ埃z我，y我∼页(。∣x我)1[y我=y我真相]=∑我=1北∇θ1[y我=y我真相]页(y我，z我∣x我)=∑我=1北1[y我=y我真相]页(y我，z我∣x我)∇θ页(y我，z我∣x我)页(y我，z我∣x我)；对数导数技巧=埃z我，y我∼页(。∣x我)1[y我=y我真相]∇θ日志⁡页(y我，z我∣x我)；对数导数技巧

每次迭代相当于首先根据以下公式选择 CoT 样本𝟙1[y=y真相]然后运行监督微调来优化生成良好CoT和答案的对数概率。STaR的性能会随着训练迭代次数的增加而提升，而生成更好CoT的“合理化”过程会加速学习。他们观察到，使用高温样本进行采样会增加通过错误推理获得正确答案的几率，而对此类数据进行LLM微调可能会损害泛化能力。对于没有真实答案的数据集，多个高温样本输出的多数投票可以作为真实答案的代理（[Wang et al. 2022](https://arxiv.org/abs/2207.00747)），从而可以使用合成样本进行训练。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/STaR-rationalization-zelikman22.png)

两种方法准确率的比较n位数字求和。通过合理化（以基本事实为条件的 CoT 生成），该模型可以学习复杂的算术任务，例如5位数求和的计算很早就出现了。（图片来源：Zelikman et al. 2022）

# 思考时间的缩放定律

到目前为止，我们已经看到大量证据表明，允许模型在推理阶段生成最终答案之前投入额外的计算资源进行推理，可以显著提升性能。一些技术，例如促使模型在得出最终答案之前生成中间推理步骤，或者训练模型在预测下一个标记之前暂停并进行反思，已被发现能够将模型性能提升到超越训练过程中获得的能力极限。这实际上为提升模型智能引入了一个全新的维度，补充了模型大小、训练计算和数据量等既定因素，正如比例定律（[Kaplan 等人，2020](https://arxiv.org/abs/2001.08361)）中所定义的那样。

最近的研究表明，优化 LLM 测试时间计算可能比扩大模型参数更有效（[Snell 等人，2024 年](https://arxiv.org/abs/2408.03314)；[Wu 等人，2025 年](https://arxiv.org/abs/2408.00724)）。较小的模型与先进的推理算法相结合，可以在成本和性能之间实现帕累托最优的权衡。

[Snell 等人 (2024)](https://arxiv.org/abs/2408.03314)评估并比较了测试时计算和预训练计算，发现它们并非 1:1 可互换。当模型能力差距较小时，测试时计算可以轻松弥补简单和中等难度问题的差距，但对于难题，其效率较低。预训练和推理的 token 预算之间的比例非常重要。只有当推理 token 数量远少于预训练 token 数量时，测试时计算才是更可取的。这表明，开发一个拥有足够预训练数据和计算能力的基础模型仍然非常关键，因为测试时计算无法解决所有问题，也无法填补模型能力的巨大差距。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/scaling-snell24.png)

（左图）通过迭代修订或并行解码，评估准确率作为测试时间计算预算的函数。（右图）比较了一个使用测试时间计算采样技巧的小型模型和一个仅使用贪婪解码的 14 倍大模型。我们可以控制测试时使用的标记预算，使得推理标记与预训练标记的比率小于 1、~=1 或 >> 1，并且只有当比率小于 1 时，测试时间计算的优势才明显。（图片来源：Snell 等人，2024）

`s1`模型（[Muennighoff & Yang 等人，2025](https://arxiv.org/abs/2501.19393)）尝试通过_预算强制_技术（即通过附加单词 强制延长路径长度`"wait"`，或通过附加思考结束标记 或 来终止模型的思考过程，从而缩短路径长度`"Final Answer:"`）来扩大 CoT 推理路径长度。他们观察到以标记为单位测量的平均思考时间与下游评估准确率之间存在明显的正相关关系。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/s1-muennighoff25.png)

测试时计算的并行和顺序扩展方法均与 s1 实验中的评估性能呈正相关。（图片：Muennighoff & Yang 等人，2025 年）

当将这种预算强制技术与控制推理轨迹长度的其他解码方法进行比较时，令人惊讶的是，简单的拒绝采样（即采样生成直到长度适合令牌预算）会导致反向缩放，这意味着更长的 CoT 会导致更差的性能。

![](https://lilianweng.github.io/posts/2025-05-01-thinking/s1-scaling-muennighoff25.png)

（左）CoT 路径长度越长，评估准确率就越高。（右）用于控制生成 CoT 路径长度的拒绝采样呈现出负向缩放关系，CoT 越长，评估准确率就越低。（图片来源：Muennighoff & Yang et al. 2025）

# 未来是什么

对测试时计算和思维链推理的探索为提升模型能力提供了新的机遇。更有趣的是，通过测试时思维，我们正在构建能够反映人类思维最佳实践的未来人工智能系统，融合适应性、灵活性、批判性反思和纠错能力。当前进展的激动人心促使我们在未来进行更多研究，以改进和深入理解我们以及我们的模型不仅如何思考，而且思考的原因。

最后，我呼吁对测试时间计算和思路链推理等开放性研究问题进行更多的研究。

- 我们能否激励模型在 RL 训练期间产生人类可读的、忠实的推理路径，同时避免奖励黑客行为？
- 如何定义奖励黑客攻击？我们能否在强化学习训练或推理过程中，在无人干预的情况下捕获奖励黑客攻击？如何防止在强化学习训练过程中出现“打地鼠”式的奖励黑客攻击修复？
- 自我修正可以在思路链中进行，也可以在多轮强化学习中明确鼓励其进行。当无法获得真实结果时，如何训练模型进行自我修正，避免出现幻觉或回归？
- 如何针对高度情境化、个性化且难以评分的任务（例如创意写作、辅导、头脑风暴）运行 RL 培训和 CoT 推广？
- 当我们在现实中部署模型时，我们不能永远增加测试时间思维，我们如何才能将性能增益平稳地转化回基础模型，同时降低推理时间成本（例如通过[蒸馏](https://arxiv.org/abs/2501.12948)）？
- 如何根据手头问题的难度，让测试时间的花费更具适应性？

# 引文

请引用此作品：

```
Weng, Lilian. "Why We Think". Lil'Log (May 2025). https://lilianweng.github.io/posts/2025-05-01-thinking/
```

或者使用 BibTex 引用：

```
@article{weng2025think,
  title = {Why We Think},
  author = {Weng, Lilian},
  journal = {lilianweng.github.io},
  year = {2025},
  month = {May},
  url = "https://lilianweng.github.io/posts/2025-05-01-thinking/"
}
```

# 参考

[1] Alex Graves. [“循环神经网络的自适应计算时间。”](https://arxiv.org/abs/1603.08983) arXiv 预印本 arXiv:1603.08983 (2016)。

[2] 王玲等，[“通过原理生成进行程序归纳：学习解决和解释代数应用题。”](https://arxiv.org/abs/1705.04146) arXiv 预印本 arXiv:1705.04146 (2017)。

[3] Karl Cobbe 等人，[“训练验证者解决数学应用题”。arXiv](https://arxiv.org/abs/2110.14168)预印本 arXiv:2110.14168 (2021)。

[4] Jason Wei 等人。[“思维链提示在大型语言模型中引发推理。”](https://arxiv.org/abs/2201.11903)。NeurIPS 2022。

[5] Maxwell Nye 等人，[“展示你的工作：基于语言模型的中级计算暂存器”](https://arxiv.org/abs/2112.00114)。arXiv 预印本 arXiv:2112.00114 (2021)。

[6] 丹尼尔·卡尼曼。_《思考，快与慢》_。Farrar, Straus and Giroux（2013）。

[7] Takeshi Kojima 等人。[“大型语言模型是零样本推理器。”](https://arxiv.org/abs/2205.11916)。NeurIPS 2022。

[8] Michihiro Yasunaga 等人，[“大型语言模型作为类比推理器”](https://arxiv.org/abs/2310.01714)。arXiv 预印本 arXiv:2310.01714 (2023)。

[9] Eric Zelikman 等人，[“STaR：通过推理进行引导推理。”](https://arxiv.org/abs/2203.14465) NeurIPS 2022。

[10] 王学智等，[“自一致性提升语言模型的思路链推理能力。”](https://arxiv.org/abs/2203.11171) ACL 2023。

[11] Ryo Kamoi 等，[“法学硕士何时才能真正纠正自己的错误？对法学硕士自我纠正的批判性调查。”](https://arxiv.org/abs/2406.01297) TACL 2024。

[12] 黄杰等人，[“大型语言模型尚无法自我纠正推理。”](https://arxiv.org/abs/2310.01798) ICLR 2024。

[13] Noah Shinn 等人，[“Reflexion：具有言语强化学习的语言代理”](https://arxiv.org/abs/2303.11366)。arXiv 预印本 arXiv:2303.11366 (2023)。

[14] 张云翔等，[“小型语言模型需要强验证器进行自我纠正推理。”](https://arxiv.org/abs/2404.17140) ACL Findings 2024。

[15] 刘浩等，[“后见之明链将语言模型与反馈相结合。”](https://arxiv.org/abs/2302.02676) arXiv 预印本 arXiv:2302.02676 (2023)。

[16] Sean Welleck 等人，[“通过学习自我纠正来生成序列。”](https://arxiv.org/abs/2211.00053) arXiv 预印本 arXiv:2211.00053 (2023)。

[17] Yuxiao Qu 等，[“递归自省：教语言模型代理如何自我提升”](https://arxiv.org/abs/2407.18219)。arXiv 预印本 arXiv:2407.18219 (2024)。

[18] Aviral Kumar 等人，[“通过强化学习训练语言模型实现自我纠正。”](https://arxiv.org/abs/2409.12917) arXiv 预印本 arXiv:2409.12917 (2024)。

[19] Hunter Lightman 等人，[“让我们逐步验证。”](https://arxiv.org/abs/2305.20050) arXiv 预印本 arXiv:2305.20050 (2023)。

[20] 谢玉玺，等。[“自我评估引导束搜索推理。”](https://arxiv.org/abs/2305.00633)。 NeurIPS 2023。

[21] 吴杨振等，[“推理尺度律：基于语言模型的计算最优推理问题求解的实证分析”](https://arxiv.org/abs/2408.00724)。ICLR 2025。

[22] 姜东伟等，[“RATIONALYST：用于改进推理的预训练过程监督”](https://arxiv.org/abs/2410.01044)。arXiv 预印本 arXiv:2410.01044 (2024)。

[23] Xuezhi Wang 和 Denny Zhou. [“无需提示的思路链式推理。”](https://arxiv.org/abs/2402.10200) arXiv 预印本 arXiv:2402.10200 (2024)。

[24] DeepSeek-AI。[“DeepSeek-V3 技术报告。”](https://arxiv.org/abs/2412.19437) arXiv 预印本 arXiv:2412.19437 (2024 年)。

[25] DeepSeek-AI. [“DeepSeek-R1：通过强化学习激励法学硕士 (LLM) 中的推理能力。”](https://arxiv.org/abs/2501.12948) . arXiv 预印本 arXiv:2501.12948 (2025).

[26] 高鲁宇，Aman Madaan，周书艳，等。[“PAL：程序辅助语言模型。”](https://arxiv.org/abs/2211.10435)。 ICML 2023。

[27] 姚顺宇等，[“ReAct：语言模型中的推理与行动协同”](https://arxiv.org/abs/2210.03629)。ICLR 2023。

[29] Bowen Baker 等人，[“监控不当行为的推理模型和促进混淆的风险。”](https://arxiv.org/abs/2503.11926) arXiv 预印本 arXiv:2503.11926 (2025)。

[30] Wojciech Zaremba 等人，[“通过交易推理时间计算实现对抗鲁棒性。”](https://arxiv.org/abs/2501.18841) arXiv 预印本 arXiv:2501.18841 (2025)。

[31] Tamera Lanham 等人。[“思路链推理中的忠诚度测量”](https://arxiv.org/abs/2307.13702)。arXiv 预印本 arXiv:2307.13702 (2023)。

[32] 王博时等，[“理解思路链提示：关于重要因素的实证研究”](https://arxiv.org/abs/2212.10001)。ACL 2023。

[33] Miles Turpin 等人，[“语言模型并非总是表达其所想：思路链提示中的不实解释。”](https://arxiv.org/abs/2305.04388) NeuriPS 2023。

[34] James Chua & Owain Evans。[“DeepSeek R1 和其他推理模型是否更可靠？”](https://arxiv.org/abs/2501.08156)。arXiv 预印本 arXiv:2501.08156 (2025)。

[35] Yanda Chen 等，[“推理模型并不总是说出它们所想的”](https://arxiv.org/abs/2505.05410)。arXiv 预印本 arXiv:2505.05410 (2025)。

[36] Edward Yeo 等人，[“揭秘法学硕士中的长链思维推理”](https://arxiv.org/abs/2502.03373)。arXiv 预印本 arXiv:2502.03373 (2025)。

[37] Mostafa Dehghani 等人。[“通用 Transformer。”](https://arxiv.org/abs/1807.03819) ICLR 2019。

[38] DeLesley Hutchins 等人，[“块循环 Transformer”](https://arxiv.org/abs/2203.07852)。NeurIPS 2022。

[39] Aydar Bulatov 等人。[“循环记忆变换器。”](https://arxiv.org/abs/2207.06881) NeuriPS 2022。

[40] Jonas Geiping 等人，[“利用潜在推理扩展测试时计算：一种递归深度方法。”](https://arxiv.org/abs/2502.05171) arXiv 预印本 arXiv:2502.05171 (2025)。

[41] Herel & Mikolov. [“语言建模的思考标记。”](https://arxiv.org/abs/2405.08644) . AITP 2023。

[42] Sachin Goyal 等人，[“三思而后行：使用暂停标记训练语言模型。”](https://arxiv.org/abs/2310.02226) ICLR 2024。

[43] Eric Zelikman 等人，[“Quiet-STaR：语言模型可以教会自己思考后再说话。”](https://arxiv.org/abs/2403.09629) arXiv 预印本 arXiv:2403.09629 (2025)。

[44] 周望春树等，[“面向以解释为潜在变量的可解释自然语言理解”](https://arxiv.org/abs/2011.05268)。NeurIPS 2020。

[45] Du Phan 等人。[“通过潜在变量推理训练思路链。”](https://arxiv.org/abs/2312.02179)。NeurIPS 2023。

[46] Yangjun Ruan 等，[“从潜在思维中推理学习”](https://arxiv.org/abs/2503.18866)。arXiv 预印本 arXiv:2503.18866 (2025)。

[47] Xuezhi Wang 等，[“语言模型中的理性增强集成”](https://arxiv.org/abs/2207.00747)。arXiv 预印本 arXiv:2207.00747 (2022)。

[48] Jared Kaplan 等人，[“神经语言模型的缩放定律”](https://arxiv.org/abs/2001.08361)。arXiv 预印本 arXiv:2001.08361 (2020)。

[49] Niklas Muennighoff & Zitong Yang 等人，[“s1：简单的测试时间缩放”](https://arxiv.org/abs/2501.19393)。arXiv 预印本 arXiv:2501.19393 (2025)。

[50] Peiyi Wang 等人，[“Math-Shepherd：无需人工注释的 LLM 逐步验证和强化”](https://arxiv.org/abs/2312.08935) arXiv 预印本 arXiv:2312.08935 (2023 年)。

[51] 刘一心等，[“改进大型语言模型微调以解决数学问题。”](https://arxiv.org/abs/2310.10047) arXiv 预印本 arXiv:2310.10047 (2023 年)。

[52] Charlie Snell 等人，[“优化扩展 LLM 测试时计算比扩展模型参数更有效。”](https://arxiv.org/abs/2408.03314) arXiv 预印本 arXiv:2408.03314 (2024)。

[53] OpenAI。o1-preview：[“学习利用法学硕士进行推理。”](https://openai.com/index/learning-to-reason-with-llms/) 2024 年 9 月 12 日。

[54] 开放人工智能。 o3：[“介绍 OpenAI o3 和 o4-mini。”](https://openai.com/index/introducing-o3-and-o4-mini/) 2025 年 4 月 16 日。