"""FastAPI åº”ç”¨ä¸»æ–‡ä»¶

æƒ…æ„Ÿè®°å¿† AI ç³»ç»Ÿ - æä¾›èŠå¤©ã€æƒ…æ„Ÿæ£€æµ‹å’Œè®°å¿†ç®¡ç† APIã€‚
"""

import json
import logging
import os
import uuid
from datetime import datetime
from typing import Any, Dict, List, Optional

from fastapi import FastAPI, HTTPException

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import httpx
import uvicorn

import chat_db
import memory_manager
import memory_extractor
import chat_processor
import hitl_handler
import image_analyzer
import mcp_manager
import mcp_tool_executor
from emotion_detector import parse_llm_output
from hitl_schema import (
    HITLAction,
    HITLDisplayRequest,
    HITLRequest,
    HITLResponseData,
    HITLResponseResult,
)


# Chat ç›¸å…³é…ç½®å’Œæ¨¡å‹
class ChatMessage(BaseModel):
    """å¯¹è¯æ¶ˆæ¯"""
    role: str  # 'user' | 'assistant'
    content: str


class ChatRequest(BaseModel):
    """å¯¹è¯è¯·æ±‚"""
    message: str
    session_id: Optional[str] = None
    history: Optional[List[ChatMessage]] = None


class ImageChatRequest(BaseModel):
    """å›¾ç‰‡åˆ†æè¯·æ±‚"""
    image: str  # Base64 encoded image (data URL or raw base64)
    session_id: Optional[str] = None
    action: str = "analyze_for_memory"  # analyze_only | analyze_for_memory


class ChatConfig(BaseModel):
    """å¯¹è¯é…ç½®"""
    api_base: str = "https://api.openai.com/v1"
    api_key: str = ""
    model: str = "gpt-4o-mini"
    system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ AI åŠ©æ‰‹ã€‚"
    max_tokens: int = 1024
    temperature: float = 0.7


def load_chat_config() -> ChatConfig:
    """ä»æ•°æ®åº“åŠ è½½å¯¹è¯é…ç½®"""
    settings = chat_db.get_all_settings()

    return ChatConfig(
        api_base=settings.get("llm.api_base", "https://api.openai.com/v1"),
        api_key=settings.get("llm.api_key", ""),
        model=settings.get("llm.model", "gpt-4o-mini"),
        system_prompt=settings.get("llm.system_prompt", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ AI åŠ©æ‰‹ã€‚"),
        max_tokens=settings.get("llm.max_tokens", 1024),
        temperature=settings.get("llm.temperature", 0.7),
    )


def is_deep_thinking_enabled() -> bool:
    """æ£€æŸ¥æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼"""
    value = chat_db.get_setting("llm.deep_thinking_enabled")
    if value is None:
        return False
    if isinstance(value, bool):
        return value
    if isinstance(value, str):
        return value.lower() in ("true", "1", "yes")
    return bool(value)


def get_deep_thinking_params(config: ChatConfig) -> dict:
    """æ ¹æ®æ·±åº¦æ€è€ƒè®¾ç½®è¿”å› LLM API å‚æ•°

    å½“æ·±åº¦æ€è€ƒå…³é—­æ—¶ï¼š
    - ä½¿ç”¨è¾ƒä½çš„æ¸©åº¦å‡å°‘å‘æ•£æ€è€ƒ
    - æ·»åŠ  reasoning_effort: "low" (OpenAI o1/o3 ç³»åˆ—)
    - ä¸æ·»åŠ  thinking å‚æ•° (Claude é»˜è®¤å…³é—­)

    å½“æ·±åº¦æ€è€ƒå¼€å¯æ—¶ï¼š
    - ä¿æŒç”¨æˆ·é…ç½®çš„æ¸©åº¦
    - æ·»åŠ  reasoning_effort: "high" (OpenAI o1/o3 ç³»åˆ—)
    - æ·»åŠ  thinking å‚æ•°å¯ç”¨æ·±åº¦æ€è€ƒ (Claude API)

    æ³¨æ„ï¼šä¸åŒ API æ”¯æŒçš„å‚æ•°ä¸åŒï¼Œä¸æ”¯æŒçš„å‚æ•°ä¼šè¢«å¿½ç•¥ã€‚

    Args:
        config: å¯¹è¯é…ç½®

    Returns:
        åŒ…å« temperature å’Œå…¶ä»–æ€è€ƒæ§åˆ¶å‚æ•°çš„å­—å…¸
    """
    if is_deep_thinking_enabled():
        return {
            "temperature": config.temperature,
            # OpenAI o1/o3 ç³»åˆ—æ¨¡å‹æ”¯æŒçš„å‚æ•°
            "reasoning_effort": "high",
            # Claude API æ”¯æŒçš„å‚æ•°ï¼ˆéœ€è¦å¯ç”¨ extended thinkingï¼‰
            "thinking": {
                "type": "enabled",
                "budget_tokens": 10000,  # é»˜è®¤ 10K tokens æ€è€ƒé¢„ç®—
            },
        }
    else:
        # æ·±åº¦æ€è€ƒå…³é—­ï¼šä½¿ç”¨å¤šç§ç­–ç•¥å‡å¼±æ€è€ƒ
        return {
            # åŸºç¡€ç­–ç•¥ï¼šé™ä½æ¸©åº¦å‡å°‘å‘æ•£æ€è€ƒ
            "temperature": min(config.temperature, 0.3),
            # OpenAI o1/o3 ç³»åˆ—ï¼šä½¿ç”¨ä½æ¨ç†åŠªåŠ›
            "reasoning_effort": "low",
            # Claude APIï¼šä¸æ·»åŠ  thinking å‚æ•°å³ä¸ºå…³é—­
            # DeepSeekï¼šreasoning æ¨¡å‹æ— æ³•å…³é—­æ€è€ƒï¼Œåªèƒ½é€šè¿‡æ¨¡å‹é€‰æ‹©
        }


def build_request_body_with_thinking(
    config: ChatConfig,
    messages: list,
    stream: bool = True,
) -> dict:
    """æ„å»ºåŒ…å«æ·±åº¦æ€è€ƒå‚æ•°çš„è¯·æ±‚ä½“

    æ ¹æ®æ·±åº¦æ€è€ƒè®¾ç½®å’Œ API å…¼å®¹æ€§æ„å»ºè¯·æ±‚ä½“ã€‚
    å¯¹äºä¸æ”¯æŒæŸäº›å‚æ•°çš„ APIï¼Œè¿™äº›å‚æ•°ä¼šè¢«å®‰å…¨å¿½ç•¥ã€‚

    Args:
        config: å¯¹è¯é…ç½®
        messages: æ¶ˆæ¯åˆ—è¡¨
        stream: æ˜¯å¦æµå¼å“åº”

    Returns:
        å®Œæ•´çš„ API è¯·æ±‚ä½“
    """
    deep_thinking_enabled = is_deep_thinking_enabled()
    deep_thinking_params = get_deep_thinking_params(config)

    request_body = {
        "model": config.model,
        "messages": messages,
        "max_tokens": config.max_tokens,
        "temperature": deep_thinking_params["temperature"],
        "stream": stream,
    }

    # æ ¹æ®æ·±åº¦æ€è€ƒçŠ¶æ€æ·»åŠ é¢å¤–å‚æ•°
    if deep_thinking_enabled:
        # æ·»åŠ  reasoning_effortï¼ˆOpenAI å…¼å®¹ï¼‰
        request_body["reasoning_effort"] = deep_thinking_params.get("reasoning_effort", "high")
        # æ·»åŠ  thinking å‚æ•°ï¼ˆClaude å…¼å®¹ï¼‰
        if "thinking" in deep_thinking_params:
            request_body["thinking"] = deep_thinking_params["thinking"]
    else:
        # å…³é—­æ—¶ä¹Ÿè®¾ç½® reasoning_effort ä¸º low
        request_body["reasoning_effort"] = deep_thinking_params.get("reasoning_effort", "low")

    # æ‰“å°è¯·æ±‚å‚æ•°æ—¥å¿—ï¼ˆä¸åŒ…å« messages å†…å®¹ï¼Œé¿å…æ—¥å¿—è¿‡å¤§ï¼‰
    log_params = {k: v for k, v in request_body.items() if k != "messages"}
    log_params["messages_count"] = len(messages)
    print(f"[DeepThinking] enabled={deep_thinking_enabled}, request_params={log_params}")

    return request_body


# æ·±åº¦æ€è€ƒå…³é—­æ—¶è¿½åŠ çš„æç¤ºè¯
DISABLE_THINKING_PROMPT_SUFFIX = "\n\nè¯·ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸éœ€è¦å±•ç¤ºæ€è€ƒè¿‡ç¨‹ã€‚ä¿æŒç®€æ´æ˜äº†ã€‚"


class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str
    service: str


class DeleteResponse(BaseModel):
    """åˆ é™¤å“åº”"""
    success: bool
    message: str


# èŠå¤©è®°å½•ç›¸å…³æ¨¡å‹
class ChatSessionInfo(BaseModel):
    """ä¼šè¯ä¿¡æ¯"""
    id: str
    created_at: str
    updated_at: str
    title: Optional[str] = None
    message_count: int = 0


class ChatMessageInfo(BaseModel):
    """æ¶ˆæ¯ä¿¡æ¯"""
    id: int
    session_id: str
    role: str
    content: str
    created_at: str


class ChatHistoryListResponse(BaseModel):
    """èŠå¤©ä¼šè¯åˆ—è¡¨å“åº”"""
    sessions: List[ChatSessionInfo]
    count: int


class ChatSessionDetailResponse(BaseModel):
    """ä¼šè¯è¯¦æƒ…å“åº”"""
    session: ChatSessionInfo
    messages: List[ChatMessageInfo]


class ChatHistoryDeleteResponse(BaseModel):
    """åˆ é™¤èŠå¤©è®°å½•å“åº”"""
    success: bool
    deleted_count: Optional[int] = None


from contextlib import asynccontextmanager


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    import asyncio
    # å¯åŠ¨æ—¶åˆå§‹åŒ–æ•°æ®åº“
    chat_db.init_database()
    # åˆå§‹åŒ– MCP ç®¡ç†å™¨ (auto-starts servers with auto_start=True)
    mcp_manager.init_mcp_manager()

    # Fetch tools list for all running MCP servers to populate cache
    pm = mcp_manager.get_process_manager()
    running_servers = pm.get_running_servers()
    if running_servers:
        print(f"[MCP] Fetching tools list for {len(running_servers)} running servers...")
        # Give servers a moment to initialize
        await asyncio.sleep(0.5)
        for server in running_servers:
            try:
                tools = await mcp_tool_executor.list_service_tools(server.name)
                print(f"[MCP] Cached {len(tools)} tools from server: {server.name}")
            except Exception as e:
                print(f"[MCP] Failed to fetch tools list from {server.name}: {e}")

    yield
    # å…³é—­æ—¶æ¸…ç† MCP æœåŠ¡è¿›ç¨‹
    stopped_count = mcp_manager.shutdown_mcp_manager()
    if stopped_count > 0:
        print(f"[MCP] Stopped {stopped_count} running MCP servers on shutdown")


# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="æƒ…æ„Ÿè®°å¿† AI ç³»ç»Ÿ",
    description="æä¾›èŠå¤©ã€æƒ…æ„Ÿæ£€æµ‹å’Œè®°å¿†ç®¡ç†åŠŸèƒ½çš„ API",
    version="0.2.0",
    lifespan=lifespan
)


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return HealthResponse(
        status="healthy",
        service="emotion-memory-system"
    )


# ============ MCP Tool Result Follow-up ============

# Prompt template for generating a final response after tool execution
MCP_FOLLOWUP_PROMPT = """ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ AI åŠ©æ‰‹ã€‚ä½ åˆšæ‰è°ƒç”¨äº†ä¸€ä¸ªå·¥å…·ï¼Œä»¥ä¸‹æ˜¯å·¥å…·è¿”å›çš„ç»“æœã€‚
è¯·æ ¹æ®å·¥å…·ç»“æœä¸ºç”¨æˆ·ç”Ÿæˆä¸€ä¸ªè‡ªç„¶ã€æœ‰ç”¨çš„å›å¤ã€‚

å·¥å…·åç§°: {tool_name}
å·¥å…·æœåŠ¡: {service_name}
è°ƒç”¨çŠ¶æ€: {status}
{result_section}

ç”¨æˆ·åŸå§‹è¯·æ±‚ä¸Šä¸‹æ–‡: {user_context}

ä»¥çº¯ JSON æ ¼å¼å›å¤:
{{"emotion":{{"primary":"neutral","category":"neutral","confidence":0.8}},"response":"ä½ çš„å›å¤","memory_update":{{"should_store":false,"entries":[]}}}}

ç›´æ¥è¾“å‡º JSON:"""


async def call_llm_with_tool_result(
    config: "ChatConfig",
    tool_result: "mcp_tool_executor.ToolCallResult",
    user_context: str,
) -> Optional[str]:
    """Call LLM again with tool result to generate a natural response.

    Args:
        config: Chat config with API credentials
        tool_result: Result from MCP tool execution
        user_context: Original user request context for reference

    Returns:
        LLM's response text, or None if failed
    """
    import time
    start = time.time()

    status = "æˆåŠŸ" if tool_result.success else "å¤±è´¥"
    if tool_result.success:
        result_section = f"å·¥å…·è¿”å›ç»“æœ:\n{tool_result.result}"
    else:
        result_section = f"é”™è¯¯ä¿¡æ¯: {tool_result.error}"

    system_prompt = MCP_FOLLOWUP_PROMPT.format(
        tool_name=tool_result.tool_name,
        service_name=tool_result.service_name,
        status=status,
        result_section=result_section,
        user_context=user_context[:500],  # Truncate to avoid overly long prompts
    )

    if not is_deep_thinking_enabled():
        system_prompt += DISABLE_THINKING_PROMPT_SUFFIX

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": "è¯·æ ¹æ®å·¥å…·ç»“æœç»™å‡ºå›å¤ã€‚"},
    ]

    api_url = f"{config.api_base.rstrip('/')}/chat/completions"
    request_body = build_request_body_with_thinking(config, messages, stream=False)
    headers = {
        "Authorization": f"Bearer {config.api_key}",
        "Content-Type": "application/json",
    }

    try:
        print(f"[MCP Followup] Calling LLM with tool result...")
        async with httpx.AsyncClient(timeout=60.0) as client:
            resp = await client.post(api_url, json=request_body, headers=headers)
            if resp.status_code != 200:
                print(f"[MCP Followup] API error: {resp.status_code}")
                return None

            data = resp.json()
            content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
            print(f"[MCP Followup] Got response in {time.time() - start:.2f}s, length={len(content)}")
            return content
    except Exception as e:
        print(f"[MCP Followup] Failed: {e}")
        return None


async def stream_chat_response(request: ChatRequest):
    """æµå¼è°ƒç”¨ LLM API å¹¶ç”Ÿæˆ SSE äº‹ä»¶

    å¦‚æœæä¾›äº† session_idï¼Œæ¶ˆæ¯ä¼šè‡ªåŠ¨ä¿å­˜åˆ°æ•°æ®åº“ã€‚
    æ”¯æŒè®°å¿†å’Œæƒ…ç»ªç³»ç»Ÿï¼ˆå¯é€šè¿‡ USE_MEMORY_EMOTION_SYSTEM ç¯å¢ƒå˜é‡å¼€å…³ï¼‰ã€‚
    """
    try:
        config = load_chat_config()
    except FileNotFoundError as e:
        yield f'event: error\ndata: {json.dumps({"type": "error", "payload": {"message": str(e)}})}\n\n'
        return
    except Exception as e:
        yield f'event: error\ndata: {json.dumps({"type": "error", "payload": {"message": f"é…ç½®åŠ è½½å¤±è´¥: {str(e)}"}})}\n\n'
        return

    # ç¡®å®š session_id
    session_id = request.session_id or str(uuid.uuid4())

    # å¦‚æœæä¾›äº† session_idï¼Œä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ°æ•°æ®åº“
    if request.session_id:
        try:
            chat_db.add_message(request.session_id, "user", request.message)
        except Exception as e:
            print(f"ä¿å­˜ç”¨æˆ·æ¶ˆæ¯å¤±è´¥: {e}")

    # æ£€æŸ¥æ˜¯å¦å¯ç”¨è®°å¿†/æƒ…ç»ªç³»ç»Ÿ
    use_memory_system = chat_processor.is_memory_emotion_enabled()

    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
    chat_context = None
    intent_result = None

    if use_memory_system:
        try:
            # å…ˆè¿è¡Œæ„å›¾è¯†åˆ«ï¼ˆä½¿ç”¨ async ç‰ˆæœ¬æ”¯æŒ Layer 2 LLMï¼‰
            if chat_processor.is_intent_recognition_enabled():
                async with httpx.AsyncClient() as intent_client:
                    intent_result = await chat_processor.recognize_intent_async(
                        message=request.message,
                        llm_client=intent_client,
                        api_base=config.api_base,
                        api_key=config.api_key,
                        model=config.model,
                    )

            # æ„å»ºå¸¦è®°å¿†çš„ä¸Šä¸‹æ–‡
            chat_context = chat_processor.build_chat_context(session_id, request.message)
            # å°†æ„å›¾è¯†åˆ«ç»“æœæ³¨å…¥åˆ°ä¸Šä¸‹æ–‡
            chat_context.intent_result = intent_result
            messages = chat_processor.build_memory_aware_messages(chat_context)
        except Exception as e:
            print(f"æ„å»ºè®°å¿†ä¸Šä¸‹æ–‡å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æ¨¡å¼: {e}")
            use_memory_system = False

    if not use_memory_system:
        # ç®€å•æ¨¡å¼ï¼šä½¿ç”¨é…ç½®çš„ system_prompt
        system_prompt = config.system_prompt
        # æ·±åº¦æ€è€ƒå…³é—­æ—¶è¿½åŠ æç¤ºè¯
        if not is_deep_thinking_enabled():
            system_prompt += DISABLE_THINKING_PROMPT_SUFFIX
        messages = [{"role": "system", "content": system_prompt}]
        if request.history:
            for msg in request.history:
                messages.append({"role": msg.role, "content": msg.content})
        messages.append({"role": "user", "content": request.message})

    # è°ƒç”¨ OpenAI å…¼å®¹ API
    api_url = f"{config.api_base.rstrip('/')}/chat/completions"

    # ä½¿ç”¨ç»Ÿä¸€çš„è¯·æ±‚ä½“æ„å»ºå‡½æ•°ï¼ŒåŒ…å«æ·±åº¦æ€è€ƒå‚æ•°ï¼ˆä¼šæ‰“å°è¯·æ±‚å‚æ•°æ—¥å¿—ï¼‰
    request_body = build_request_body_with_thinking(config, messages, stream=True)

    headers = {
        "Authorization": f"Bearer {config.api_key}",
        "Content-Type": "application/json"
    }

    # ç”¨äºæ”¶é›†å®Œæ•´çš„åŠ©æ‰‹å“åº”
    assistant_response = ""
    detected_emotion = None

    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            async with client.stream(
                "POST",
                api_url,
                json=request_body,
                headers=headers
            ) as response:
                if response.status_code != 200:
                    error_text = await response.aread()
                    yield f'event: error\ndata: {json.dumps({"type": "error", "payload": {"message": f"API è¯·æ±‚å¤±è´¥: {response.status_code} - {error_text.decode()}"}})}\n\n'
                    return

                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data = line[6:]
                        if data.strip() == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data)
                            delta = chunk.get("choices", [{}])[0].get("delta", {})
                            content = delta.get("content", "")
                            if content:
                                assistant_response += content
                                # åœ¨è®°å¿†æ¨¡å¼ä¸‹ï¼Œå…ˆæ”¶é›†å®Œæ•´å“åº”å†è§£æ
                                if not use_memory_system:
                                    yield f'event: text\ndata: {json.dumps({"type": "text", "payload": {"content": content}})}\n\n'
                        except json.JSONDecodeError:
                            continue

        # å¤„ç†å®Œæ•´å“åº”
        # æ‰“å°å“åº”çŠ¶æ€æ—¥å¿—
        has_thinking_tags = "<thinking>" in assistant_response.lower() if assistant_response else False
        deep_thinking_mode = is_deep_thinking_enabled()
        print(f"[DeepThinking] Response received: length={len(assistant_response)}, has_thinking_tags={has_thinking_tags}, deep_thinking_enabled={deep_thinking_mode}")

        # åº”ç”¨ thinking æ ‡ç­¾è¿‡æ»¤ï¼ˆæ·±åº¦æ€è€ƒå…³é—­æ—¶ï¼‰
        final_response = chat_processor.filter_thinking_tags(assistant_response)

        # å¦‚æœè¿›è¡Œäº†è¿‡æ»¤ï¼Œæ‰“å°è¿‡æ»¤ç»“æœ
        if len(final_response) != len(assistant_response):
            print(f"[DeepThinking] Thinking tags filtered: original_length={len(assistant_response)}, filtered_length={len(final_response)}")

        hitl_request = None

        if use_memory_system and assistant_response and chat_context:
            try:
                # è§£æ LLM çš„ JSON è¾“å‡º
                chat_result = chat_processor.process_llm_response(assistant_response, chat_context)
                final_response = chat_result.response
                detected_emotion = chat_result.emotion

                # æ£€æŸ¥æ˜¯å¦æœ‰ HITL è¯·æ±‚
                if chat_processor.is_hitl_enabled():
                    print(f"[HITL] Checking for HITL in response (length={len(assistant_response)})")
                    hitl_request = hitl_handler.extract_hitl_from_llm_response(assistant_response)
                    if hitl_request:
                        if isinstance(hitl_request, HITLDisplayRequest):
                            print(f"[HITL] Found visual_display request: id={hitl_request.id}, title={hitl_request.title}, displays={len(hitl_request.displays)}")
                            hitl_handler.store_display_request(hitl_request, session_id)
                        else:
                            print(f"[HITL] Found form request: id={hitl_request.id}, title={hitl_request.title}, fields={len(hitl_request.fields)}")
                            hitl_handler.store_hitl_request(hitl_request, session_id)
                    else:
                        print("[HITL] No HITL request found in response")

                # å‘é€è§£æåçš„å“åº”æ–‡æœ¬
                yield f'event: text\ndata: {json.dumps({"type": "text", "payload": {"content": final_response}})}\n\n'

                # å‘é€æƒ…ç»ªä¿¡æ¯
                if detected_emotion:
                    yield f'event: emotion\ndata: {json.dumps({"type": "emotion", "payload": {"primary": detected_emotion.primary, "category": detected_emotion.category, "confidence": detected_emotion.confidence}})}\n\n'

                # å‘é€è®°å¿†ä¿å­˜äº‹ä»¶
                if chat_result.memories_to_store:
                    memory_payload = {
                        "count": len(chat_result.memories_to_store),
                        "entries": chat_result.memories_to_store,
                    }
                    yield f'event: memory_saved\ndata: {json.dumps({"type": "memory_saved", "payload": memory_payload})}\n\n'

                # å‘é€ HITL è¯·æ±‚äº‹ä»¶
                if hitl_request:
                    print(f"[HITL] Sending HITL SSE event for request {hitl_request.id}, type={hitl_request.type}")
                    if isinstance(hitl_request, HITLDisplayRequest):
                        # visual_display ç±»å‹
                        hitl_payload = {
                            "id": hitl_request.id,
                            "type": hitl_request.type,
                            "title": hitl_request.title,
                            "description": hitl_request.description,
                            "displays": [
                                {
                                    "type": d.type.value if hasattr(d.type, 'value') else str(d.type),
                                    "data": d.data,
                                }
                                for d in hitl_request.displays
                            ],
                            "dismiss_label": hitl_request.dismiss_label,
                            "session_id": session_id,
                        }
                    else:
                        # form ç±»å‹
                        hitl_payload = {
                            "id": hitl_request.id,
                            "type": hitl_request.type,
                            "title": hitl_request.title,
                            "description": hitl_request.description,
                            "fields": [
                                {
                                    "name": f.name,
                                    "type": f.type.value,
                                    "label": f.label,
                                    "required": f.required,
                                    "placeholder": f.placeholder,
                                    "default": f.default,
                                    "options": [{"value": o.value, "label": o.label} for o in f.options] if f.options else None,
                                    "min": f.min,
                                    "max": f.max,
                                    "step": f.step,
                                }
                                for f in hitl_request.fields
                            ],
                            "actions": {
                                "approve": {"label": hitl_request.actions.approve.label, "style": hitl_request.actions.approve.style.value},
                                "edit": {"label": hitl_request.actions.edit.label, "style": hitl_request.actions.edit.style.value},
                                "reject": {"label": hitl_request.actions.reject.label, "style": hitl_request.actions.reject.style.value},
                            },
                            "session_id": session_id,
                        }
                    yield f'event: hitl\ndata: {json.dumps({"type": "hitl", "payload": hitl_payload})}\n\n'

                # æ£€æŸ¥æ˜¯å¦æœ‰ MCP å·¥å…·è°ƒç”¨è¯·æ±‚
                tool_call_request = chat_processor.extract_tool_call(assistant_response)
                if tool_call_request:
                    print(f"[MCP] Found tool_call: name={tool_call_request.name}, method={tool_call_request.method}")
                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    tool_result = await mcp_tool_executor.execute_mcp_tool(
                        service_name=tool_call_request.name,
                        method=tool_call_request.method,
                        arguments=tool_call_request.arguments,
                    )
                    # å‘é€å·¥å…·è°ƒç”¨ç»“æœ SSE äº‹ä»¶
                    tool_result_payload = {
                        "success": tool_result.success,
                        "tool_name": tool_result.tool_name,
                        "service_name": tool_result.service_name,
                        "result": tool_result.result,
                        "error": tool_result.error,
                    }
                    yield f'event: tool_result\ndata: {json.dumps({"type": "tool_result", "payload": tool_result_payload})}\n\n'
                    print(f"[MCP] Tool result sent: success={tool_result.success}")

                    # äºŒæ¬¡è°ƒç”¨ LLMï¼Œå°†å·¥å…·ç»“æœè½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€å›å¤
                    followup_response = await call_llm_with_tool_result(
                        config=config,
                        tool_result=tool_result,
                        user_context=request.message,
                    )
                    if followup_response:
                        # è§£æ followup å“åº”
                        try:
                            followup_parsed = parse_llm_output(followup_response)
                            followup_text = followup_parsed.response
                            print(f"[MCP Followup] Parsed response: {len(followup_text)} chars")
                        except Exception:
                            followup_text = chat_processor.extract_response_text(followup_response)
                            print(f"[MCP Followup] Using raw response: {len(followup_text)} chars")

                        # å‘é€ followup å“åº”
                        yield f'event: text\ndata: {json.dumps({"type": "text", "payload": {"content": followup_text}})}\n\n'
                        # æ›´æ–° final_response ç”¨äºä¿å­˜åˆ°æ•°æ®åº“
                        final_response = followup_text

            except Exception as e:
                print(f"è§£æ LLM å“åº”å¤±è´¥: {e}")
                # å›é€€ï¼šç›´æ¥ä½¿ç”¨åŸå§‹å“åº”
                final_response = chat_processor.extract_response_text(assistant_response)
                yield f'event: text\ndata: {json.dumps({"type": "text", "payload": {"content": final_response}})}\n\n'

        # ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯åˆ°æ•°æ®åº“
        if request.session_id and final_response:
            try:
                chat_db.add_message(request.session_id, "assistant", final_response)
            except Exception as e:
                print(f"ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯å¤±è´¥: {e}")

        yield f'event: done\ndata: {json.dumps({"type": "done"})}\n\n'

    except httpx.TimeoutException:
        yield f'event: error\ndata: {json.dumps({"type": "error", "payload": {"message": "API è¯·æ±‚è¶…æ—¶"}})}\n\n'
    except httpx.ConnectError:
        yield f'event: error\ndata: {json.dumps({"type": "error", "payload": {"message": "æ— æ³•è¿æ¥åˆ° API æœåŠ¡å™¨"}})}\n\n'
    except Exception as e:
        yield f'event: error\ndata: {json.dumps({"type": "error", "payload": {"message": f"è¯·æ±‚é”™è¯¯: {str(e)}"}})}\n\n'


@app.post("/chat")
async def chat(request: ChatRequest):
    """å¯¹è¯æ¥å£ - è¿”å› SSE æµå¼å“åº”"""
    return StreamingResponse(
        stream_chat_response(request),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


# ============ å›¾ç‰‡åˆ†æ API ============

async def stream_image_analysis(request: ImageChatRequest):
    """åˆ†æå›¾ç‰‡å¹¶æå–æ–‡æœ¬ï¼Œå¯é€‰ç”Ÿæˆè®°å¿†ä¿å­˜ HITL è¯·æ±‚ã€‚

    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    - analyze_only: ä»…åˆ†æå›¾ç‰‡è¿”å›æ–‡æœ¬
    - analyze_for_memory: åˆ†æåç”Ÿæˆ HITL è®©ç”¨æˆ·ç¡®è®¤ä¿å­˜åˆ°è®°å¿†
    """
    session_id = request.session_id or str(uuid.uuid4())

    try:
        # Step 1: ä½¿ç”¨ Vision API åˆ†æå›¾ç‰‡
        extracted_text = await image_analyzer.analyze_image_text(request.image)

        # å‘é€æå–çš„æ–‡æœ¬
        text_content = f"ğŸ“· å›¾ç‰‡æ–‡æœ¬è¯†åˆ«ç»“æœï¼š\n\n{extracted_text}"
        yield f'event: text\ndata: {json.dumps({"type": "text", "payload": {"content": text_content}})}\n\n'

        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ–‡æœ¬å†…å®¹
        if not image_analyzer.has_text_content(extracted_text):
            no_text_msg = "\n\nå›¾ç‰‡ä¸­æœªè¯†åˆ«åˆ°å¯ä¿å­˜çš„æ–‡æœ¬å†…å®¹ã€‚"
            yield f'event: text\ndata: {json.dumps({"type": "text", "payload": {"content": no_text_msg}})}\n\n'
            yield f'event: done\ndata: {json.dumps({"type": "done"})}\n\n'
            return

        # Step 2: å¦‚æœæ˜¯ analyze_for_memory æ¨¡å¼ï¼Œå°è¯•æå–è®°å¿†ä¿¡æ¯
        if request.action == "analyze_for_memory":
            try:
                # ä½¿ç”¨ memory_extractor ä»æ–‡æœ¬ä¸­æå–è®°å¿†ä¿¡æ¯
                memory_result = await memory_extractor.extract_memory_from_message(
                    content=extracted_text,
                    role="user",
                    source="image",  # æ ‡è®°æ¥æºä¸ºå›¾ç‰‡OCR
                )

                # è°ƒè¯•æ—¥å¿—
                if memory_result:
                    print(f"[ImageAnalysis] memory_result: key={memory_result.key}, category={memory_result.category}, confidence={memory_result.confidence}")
                else:
                    print("[ImageAnalysis] memory_result is None")

                if memory_result and memory_result.confidence >= 0.3:
                    # ç”Ÿæˆ HITL è¯·æ±‚è®©ç”¨æˆ·ç¡®è®¤
                    from hitl_schema import (
                        HITLRequest as HITLRequestModel,
                        HITLField,
                        HITLFieldType,
                        HITLOption,
                        HITLActions,
                        HITLActionButton,
                        HITLActionStyle,
                        HITLContext,
                    )

                    # åŸºç¡€å­—æ®µ
                    fields = [
                        HITLField(
                            name="key",
                            type=HITLFieldType.TEXT,
                            label="è®°å¿†åç§°",
                            required=True,
                            placeholder="ä¾‹å¦‚ï¼šä¼šè®®ç¬”è®°ã€ä¹¦ç±æ‘˜å½•ã€å‘¨äº”èšé¤",
                            default=memory_result.key,
                        ),
                        HITLField(
                            name="value",
                            type=HITLFieldType.TEXTAREA,
                            label="è®°å¿†å†…å®¹",
                            required=True,
                            placeholder="æå–çš„æ–‡æœ¬å†…å®¹",
                            default=memory_result.value,
                        ),
                        HITLField(
                            name="category",
                            type=HITLFieldType.SELECT,
                            label="ç±»åˆ«",
                            required=True,
                            default=memory_result.category,
                            options=[
                                HITLOption(value="preference", label="åå¥½"),
                                HITLOption(value="fact", label="äº‹å®"),
                                HITLOption(value="pattern", label="æ¨¡å¼"),
                                HITLOption(value="plan", label="è®¡åˆ’"),
                            ],
                        ),
                    ]

                    # å¦‚æœæ˜¯è®¡åˆ’ç±»åˆ«ï¼Œæ·»åŠ è®¡åˆ’ç‰¹å®šå­—æ®µ
                    if memory_result.category == "plan":
                        fields.extend([
                            HITLField(
                                name="target_time",
                                type=HITLFieldType.TEXT,
                                label="ç›®æ ‡æ—¶é—´",
                                required=True,
                                placeholder="ä¾‹å¦‚ï¼š2025-01-28T14:00:00",
                                default=memory_result.target_time or "",
                            ),
                            HITLField(
                                name="location",
                                type=HITLFieldType.TEXT,
                                label="åœ°ç‚¹",
                                required=False,
                                placeholder="ä¾‹å¦‚ï¼šä¼šè®®å®¤Aã€æ˜Ÿå·´å…‹",
                                default=memory_result.location or "",
                            ),
                            HITLField(
                                name="participants",
                                type=HITLFieldType.TEXT,
                                label="å‚ä¸è€…",
                                required=False,
                                placeholder="ä¾‹å¦‚ï¼šå¼ ä¸‰,æå››",
                                default=memory_result.participants or "",
                            ),
                        ])

                    # ç¡®å®š intent å’Œ memory_category
                    if memory_result.category == "plan":
                        intent = "collect_plan"
                        hitl_type = "image_plan_confirm"
                        hitl_title = "ä¿å­˜è®¡åˆ’åˆ°æ—¥ç¨‹"
                        hitl_description = "AI ä»å›¾ç‰‡ä¸­è¯†åˆ«åˆ°è®¡åˆ’å®‰æ’ï¼Œè¯·ç¡®è®¤æ˜¯å¦ä¿å­˜åˆ°æ—¥ç¨‹ã€‚"
                    else:
                        intent = "collect_preference"
                        hitl_type = "image_memory_confirm"
                        hitl_title = "ä¿å­˜å›¾ç‰‡å†…å®¹åˆ°é•¿æœŸè®°å¿†"
                        hitl_description = "AI ä»å›¾ç‰‡ä¸­æå–äº†ä»¥ä¸‹ä¿¡æ¯ï¼Œè¯·ç¡®è®¤æ˜¯å¦ä¿å­˜ã€‚"

                    hitl_request = HITLRequestModel(
                        id=str(uuid.uuid4()),
                        type=hitl_type,
                        title=hitl_title,
                        description=hitl_description,
                        fields=fields,
                        actions=HITLActions(
                            approve=HITLActionButton(label="ä¿å­˜", style=HITLActionStyle.PRIMARY),
                            edit=HITLActionButton(label="ç¼–è¾‘", style=HITLActionStyle.DEFAULT),
                            reject=HITLActionButton(label="è·³è¿‡", style=HITLActionStyle.SECONDARY),
                        ),
                        context=HITLContext(
                            intent=intent,
                            memory_category=memory_result.category,
                        ),
                    )

                    # å­˜å‚¨ HITL è¯·æ±‚
                    hitl_handler.store_hitl_request(hitl_request, session_id)

                    # å‘é€ HITL äº‹ä»¶
                    hitl_payload = {
                        "id": hitl_request.id,
                        "type": hitl_request.type,
                        "title": hitl_request.title,
                        "description": hitl_request.description,
                        "fields": [
                            {
                                "name": f.name,
                                "type": f.type.value,
                                "label": f.label,
                                "required": f.required,
                                "placeholder": f.placeholder,
                                "default": f.default,
                                "options": [{"value": o.value, "label": o.label} for o in f.options] if f.options else None,
                            }
                            for f in hitl_request.fields
                        ],
                        "actions": {
                            "approve": {"label": hitl_request.actions.approve.label, "style": hitl_request.actions.approve.style.value},
                            "edit": {"label": hitl_request.actions.edit.label, "style": hitl_request.actions.edit.style.value},
                            "reject": {"label": hitl_request.actions.reject.label, "style": hitl_request.actions.reject.style.value},
                        },
                        "session_id": session_id,
                    }
                    yield f'event: hitl\ndata: {json.dumps({"type": "hitl", "payload": hitl_payload})}\n\n'
                else:
                    no_memory_msg = "\n\næœªèƒ½ä»æ–‡æœ¬ä¸­æå–å‡ºé€‚åˆä¿å­˜çš„è®°å¿†ä¿¡æ¯ã€‚"
                    yield f'event: text\ndata: {json.dumps({"type": "text", "payload": {"content": no_memory_msg}})}\n\n'

            except Exception as e:
                print(f"Memory extraction failed: {e}")
                error_msg = f"\n\nè®°å¿†æå–å¤±è´¥: {str(e)}"
                yield f'event: text\ndata: {json.dumps({"type": "text", "payload": {"content": error_msg}})}\n\n'

        yield f'event: done\ndata: {json.dumps({"type": "done"})}\n\n'

    except ValueError as e:
        yield f'event: error\ndata: {json.dumps({"type": "error", "payload": {"message": str(e)}})}\n\n'
    except Exception as e:
        yield f'event: error\ndata: {json.dumps({"type": "error", "payload": {"message": f"å›¾ç‰‡åˆ†æå¤±è´¥: {str(e)}"}})}\n\n'


@app.post("/chat/image")
async def chat_image(request: ImageChatRequest):
    """å›¾ç‰‡åˆ†ææ¥å£ - è¿”å› SSE æµå¼å“åº”

    ä½¿ç”¨ Vision LLM åˆ†æå›¾ç‰‡ä¸­çš„æ–‡æœ¬å†…å®¹ï¼Œå¯é€‰ä¿å­˜åˆ°é•¿æœŸè®°å¿†ã€‚
    """
    return StreamingResponse(
        stream_image_analysis(request),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


# ============ èŠå¤©å†å²è®°å½• API ============

@app.get("/chat/history", response_model=ChatHistoryListResponse)
async def get_chat_history(limit: int = 100, offset: int = 0):
    """è·å–èŠå¤©ä¼šè¯åˆ—è¡¨

    è¿”å›æ‰€æœ‰ä¼šè¯ï¼ŒæŒ‰ updated_at é™åºæ’åˆ—ã€‚
    """
    try:
        sessions = chat_db.list_sessions(limit=limit, offset=offset)
        session_list = [
            ChatSessionInfo(
                id=s["id"],
                created_at=s["created_at"],
                updated_at=s["updated_at"],
                title=s.get("title"),
                message_count=s.get("message_count", 0)
            )
            for s in sessions
        ]
        return ChatHistoryListResponse(sessions=session_list, count=len(session_list))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä¼šè¯åˆ—è¡¨å¤±è´¥: {str(e)}")


@app.get("/chat/history/{session_id}", response_model=ChatSessionDetailResponse)
async def get_chat_session(session_id: str):
    """è·å–ç‰¹å®šä¼šè¯çš„è¯¦æƒ…å’Œæ¶ˆæ¯åˆ—è¡¨"""
    try:
        result = chat_db.get_session_with_messages(session_id)
        if not result:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")

        session = result["session"]
        messages = result["messages"]

        # è®¡ç®—æ¶ˆæ¯æ•°
        message_count = len(messages)

        session_info = ChatSessionInfo(
            id=session["id"],
            created_at=session["created_at"],
            updated_at=session["updated_at"],
            title=session.get("title"),
            message_count=message_count
        )

        message_list = [
            ChatMessageInfo(
                id=m["id"],
                session_id=m["session_id"],
                role=m["role"],
                content=m["content"],
                created_at=m["created_at"]
            )
            for m in messages
        ]

        return ChatSessionDetailResponse(session=session_info, messages=message_list)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä¼šè¯è¯¦æƒ…å¤±è´¥: {str(e)}")


@app.delete("/chat/history/{session_id}", response_model=ChatHistoryDeleteResponse)
async def delete_chat_session(session_id: str):
    """åˆ é™¤ç‰¹å®šä¼šè¯åŠå…¶æ‰€æœ‰æ¶ˆæ¯"""
    try:
        success = chat_db.delete_session(session_id)
        if not success:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
        return ChatHistoryDeleteResponse(success=True)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤ä¼šè¯å¤±è´¥: {str(e)}")


@app.delete("/chat/history", response_model=ChatHistoryDeleteResponse)
async def clear_chat_history():
    """æ¸…ç©ºæ‰€æœ‰èŠå¤©è®°å½•"""
    try:
        deleted_count = chat_db.delete_all_sessions()
        return ChatHistoryDeleteResponse(success=True, deleted_count=deleted_count)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ¸…ç©ºèŠå¤©è®°å½•å¤±è´¥: {str(e)}")


# ============ Memory Management API ============

class MemoryEntryInfo(BaseModel):
    """è®°å¿†æ¡ç›®ä¿¡æ¯"""
    id: str
    session_id: Optional[str] = None
    category: str
    key: str
    value: Any
    confidence: float
    source: str
    created_at: str
    last_accessed: str
    access_count: int
    # Plan-specific fields (only when category == 'plan')
    target_time: Optional[str] = None
    reminder_offset_minutes: Optional[int] = None
    repeat_type: Optional[str] = None
    plan_status: Optional[str] = None
    snooze_until: Optional[str] = None


class MemoryEntryCreateRequest(BaseModel):
    """åˆ›å»ºè®°å¿†æ¡ç›®è¯·æ±‚"""
    category: str
    key: str
    value: Any
    confidence: float = 0.9
    source: str = "user_stated"
    # Plan-specific fields (only when category == 'plan')
    target_time: Optional[str] = None
    reminder_offset_minutes: Optional[int] = None
    repeat_type: Optional[str] = None


class MemoryEntryUpdateRequest(BaseModel):
    """æ›´æ–°è®°å¿†æ¡ç›®è¯·æ±‚"""
    key: Optional[str] = None
    value: Optional[Any] = None
    category: Optional[str] = None
    confidence: Optional[float] = None
    # Plan-specific fields (only when category == 'plan')
    target_time: Optional[str] = None
    reminder_offset_minutes: Optional[int] = None
    repeat_type: Optional[str] = None


class MemoryListResponse(BaseModel):
    """è®°å¿†åˆ—è¡¨å“åº”"""
    memories: List[MemoryEntryInfo]
    total: int


class MemoryBatchDeleteRequest(BaseModel):
    """æ‰¹é‡åˆ é™¤è¯·æ±‚"""
    ids: List[str]


class MemoryBatchDeleteResponse(BaseModel):
    """æ‰¹é‡åˆ é™¤å“åº”"""
    success: bool
    deleted_count: int


class MemoryImportRequest(BaseModel):
    """å¯¼å…¥è®°å¿†è¯·æ±‚"""
    memories: List[MemoryEntryCreateRequest]
    mode: str = "skip"  # skip | overwrite | merge


class MemoryImportResponse(BaseModel):
    """å¯¼å…¥è®°å¿†å“åº”"""
    success: bool
    imported_count: int
    skipped_count: int


class WorkingMemoryInfo(BaseModel):
    """å·¥ä½œè®°å¿†ä¿¡æ¯"""
    session_id: str
    current_topic: Optional[str] = None
    context_variables: Dict[str, Any] = {}
    turn_count: int
    last_emotion: Optional[str] = None
    created_at: str
    updated_at: str


def _memory_entry_to_info(entry: memory_manager.MemoryEntry) -> MemoryEntryInfo:
    """Convert MemoryEntry to MemoryEntryInfo."""
    return MemoryEntryInfo(
        id=entry.id,
        session_id=entry.session_id,
        category=entry.category,
        key=entry.key,
        value=entry.value,
        confidence=entry.confidence,
        source=entry.source,
        created_at=entry.created_at.isoformat(),
        last_accessed=entry.last_accessed.isoformat(),
        access_count=entry.access_count,
        target_time=entry.target_time.isoformat() if entry.target_time else None,
        reminder_offset_minutes=entry.reminder_offset_minutes,
        repeat_type=entry.repeat_type,
        plan_status=entry.plan_status,
        snooze_until=entry.snooze_until.isoformat() if entry.snooze_until else None,
    )


@app.get("/memory/long-term", response_model=MemoryListResponse)
async def list_long_term_memories(
    category: Optional[str] = None,
    limit: int = 100,
    offset: int = 0,
    order_by: str = "last_accessed",
):
    """è·å–é•¿æœŸè®°å¿†åˆ—è¡¨

    æ”¯æŒæŒ‰ç±»åˆ«ç­›é€‰å’Œåˆ†é¡µã€‚
    """
    try:
        entries = memory_manager.list_memory_entries(
            category=category,
            limit=limit,
            offset=offset,
            order_by=order_by,
        )
        total = memory_manager.count_memory_entries(category=category)

        memories = [_memory_entry_to_info(e) for e in entries]
        return MemoryListResponse(memories=memories, total=total)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–è®°å¿†åˆ—è¡¨å¤±è´¥: {str(e)}")


@app.get("/memory/long-term/{memory_id}", response_model=MemoryEntryInfo)
async def get_long_term_memory(memory_id: str):
    """è·å–ç‰¹å®šé•¿æœŸè®°å¿†è¯¦æƒ…"""
    try:
        entry = memory_manager.get_memory_entry(memory_id)
        if not entry:
            raise HTTPException(status_code=404, detail="è®°å¿†ä¸å­˜åœ¨")
        return _memory_entry_to_info(entry)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–è®°å¿†å¤±è´¥: {str(e)}")


@app.post("/memory/long-term", response_model=MemoryEntryInfo)
async def create_long_term_memory(request: MemoryEntryCreateRequest):
    """æ‰‹åŠ¨åˆ›å»ºé•¿æœŸè®°å¿†æ¡ç›®

    å¦‚æœ category æ˜¯ 'plan'ï¼Œåˆ™åˆ›å»ºè®¡åˆ’æ¡ç›®å¹¶è®¾ç½®æé†’ç›¸å…³å­—æ®µã€‚
    """
    try:
        if request.category == 'plan' and request.target_time:
            # Create as plan entry with time-specific fields
            from datetime import datetime as dt
            target_time = dt.fromisoformat(request.target_time.replace("Z", "+00:00"))
            plan = memory_manager.create_plan(
                title=request.key,
                description=request.value if isinstance(request.value, str) else str(request.value),
                target_time=target_time,
                reminder_offset_minutes=request.reminder_offset_minutes if request.reminder_offset_minutes is not None else 10,
                repeat_type=request.repeat_type or "none",
            )
            # Get the memory entry to return full info
            entry = memory_manager.get_memory_entry(plan.id)
            if entry:
                return _memory_entry_to_info(entry)
            # Fallback: construct from plan
            return MemoryEntryInfo(
                id=plan.id,
                category='plan',
                key=plan.title,
                value=plan.description or '',
                confidence=1.0,
                source='user_stated',
                created_at=plan.created_at.isoformat(),
                last_accessed=plan.updated_at.isoformat(),
                access_count=0,
                target_time=plan.target_time.isoformat(),
                reminder_offset_minutes=plan.reminder_offset_minutes,
                repeat_type=plan.repeat_type,
                plan_status=plan.status,
            )
        else:
            entry = memory_manager.create_memory_entry(
                category=request.category,
                key=request.key,
                value=request.value,
                confidence=request.confidence,
                source=request.source,
            )
            return _memory_entry_to_info(entry)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"åˆ›å»ºè®°å¿†å¤±è´¥: {str(e)}")


@app.put("/memory/long-term/{memory_id}", response_model=MemoryEntryInfo)
async def update_long_term_memory(memory_id: str, request: MemoryEntryUpdateRequest):
    """æ›´æ–°é•¿æœŸè®°å¿†æ¡ç›®

    å¦‚æœæ˜¯ plan ç±»åˆ«ï¼ŒåŒæ—¶æ›´æ–°è®¡åˆ’ç›¸å…³å­—æ®µã€‚
    """
    try:
        # Check if this is a plan entry
        existing = memory_manager.get_memory_entry(memory_id)
        if existing and existing.category == 'plan':
            # Update as plan entry
            from datetime import datetime as dt
            target_time = None
            if request.target_time:
                target_time = dt.fromisoformat(request.target_time.replace("Z", "+00:00"))

            plan = memory_manager.update_plan(
                plan_id=memory_id,
                title=request.key,
                description=request.value if isinstance(request.value, str) else str(request.value) if request.value else None,
                target_time=target_time,
                reminder_offset_minutes=request.reminder_offset_minutes,
                repeat_type=request.repeat_type,
            )
            if not plan:
                raise HTTPException(status_code=404, detail="è®°å¿†ä¸å­˜åœ¨")
            entry = memory_manager.get_memory_entry(memory_id)
            if entry:
                return _memory_entry_to_info(entry)
            raise HTTPException(status_code=404, detail="è®°å¿†ä¸å­˜åœ¨")
        else:
            entry = memory_manager.update_memory_entry(
                memory_id=memory_id,
                key=request.key,
                value=request.value,
                category=request.category,
                confidence=request.confidence,
            )
            if not entry:
                raise HTTPException(status_code=404, detail="è®°å¿†ä¸å­˜åœ¨")
            return _memory_entry_to_info(entry)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"æ›´æ–°è®°å¿†å¤±è´¥: {str(e)}")


@app.delete("/memory/long-term/{memory_id}", response_model=DeleteResponse)
async def delete_long_term_memory(memory_id: str):
    """åˆ é™¤ç‰¹å®šé•¿æœŸè®°å¿†"""
    try:
        success = memory_manager.delete_memory_entry(memory_id)
        if not success:
            raise HTTPException(status_code=404, detail="è®°å¿†ä¸å­˜åœ¨")
        return DeleteResponse(success=True, message="è®°å¿†åˆ é™¤æˆåŠŸ")
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤è®°å¿†å¤±è´¥: {str(e)}")


@app.delete("/memory/long-term", response_model=MemoryBatchDeleteResponse)
async def clear_all_long_term_memories():
    """æ¸…ç©ºæ‰€æœ‰é•¿æœŸè®°å¿†"""
    try:
        deleted_count = memory_manager.delete_all_memories()
        return MemoryBatchDeleteResponse(success=True, deleted_count=deleted_count)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ¸…ç©ºè®°å¿†å¤±è´¥: {str(e)}")


@app.post("/memory/long-term/batch-delete", response_model=MemoryBatchDeleteResponse)
async def batch_delete_long_term_memories(request: MemoryBatchDeleteRequest):
    """æ‰¹é‡åˆ é™¤é•¿æœŸè®°å¿†"""
    try:
        deleted_count = 0
        for memory_id in request.ids:
            if memory_manager.delete_memory_entry(memory_id):
                deleted_count += 1
        return MemoryBatchDeleteResponse(success=True, deleted_count=deleted_count)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡åˆ é™¤å¤±è´¥: {str(e)}")


@app.get("/memory/long-term/export")
async def export_long_term_memories():
    """å¯¼å‡ºæ‰€æœ‰é•¿æœŸè®°å¿†ä¸º JSON"""
    try:
        entries = memory_manager.list_memory_entries(limit=10000)
        export_data = {
            "version": "1.0",
            "exported_at": datetime.now().isoformat(),
            "memories": [
                {
                    "category": e.category,
                    "key": e.key,
                    "value": e.value,
                    "confidence": e.confidence,
                    "source": e.source,
                }
                for e in entries
            ]
        }
        return export_data
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯¼å‡ºè®°å¿†å¤±è´¥: {str(e)}")


@app.post("/memory/long-term/import", response_model=MemoryImportResponse)
async def import_long_term_memories(request: MemoryImportRequest):
    """å¯¼å…¥é•¿æœŸè®°å¿†

    mode:
    - skip: è·³è¿‡å·²å­˜åœ¨çš„ï¼ˆæŒ‰ key åˆ¤æ–­ï¼‰
    - overwrite: è¦†ç›–å·²å­˜åœ¨çš„
    - merge: åˆå¹¶ï¼ˆæ›´æ–° confidence ä¸ºè¾ƒé«˜å€¼ï¼‰
    """
    try:
        imported_count = 0
        skipped_count = 0

        # Get existing keys for duplicate detection
        existing = memory_manager.list_memory_entries(limit=10000)
        existing_keys = {e.key: e for e in existing}

        for mem in request.memories:
            if mem.key in existing_keys:
                if request.mode == "skip":
                    skipped_count += 1
                    continue
                elif request.mode == "overwrite":
                    memory_manager.delete_memory_entry(existing_keys[mem.key].id)
                elif request.mode == "merge":
                    existing_entry = existing_keys[mem.key]
                    memory_manager.update_memory_entry(
                        existing_entry.id,
                        value=mem.value,
                        confidence=max(existing_entry.confidence, mem.confidence),
                    )
                    imported_count += 1
                    continue

            memory_manager.create_memory_entry(
                category=mem.category,
                key=mem.key,
                value=mem.value,
                confidence=mem.confidence,
                source=mem.source,
            )
            imported_count += 1

        return MemoryImportResponse(
            success=True,
            imported_count=imported_count,
            skipped_count=skipped_count,
        )
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"å¯¼å…¥è®°å¿†å¤±è´¥: {str(e)}")


class WorkingMemoryListResponse(BaseModel):
    """å·¥ä½œè®°å¿†åˆ—è¡¨å“åº”"""
    memories: List[WorkingMemoryInfo]
    total: int


@app.get("/memory/working", response_model=WorkingMemoryListResponse)
async def list_working_memories(limit: int = 100):
    """è·å–æ‰€æœ‰æ´»è·ƒçš„å·¥ä½œè®°å¿†åˆ—è¡¨"""
    try:
        memories = memory_manager.list_working_memories(limit=limit)
        result = [
            WorkingMemoryInfo(
                session_id=wm.session_id,
                current_topic=wm.current_topic,
                context_variables=wm.context_variables,
                turn_count=wm.turn_count,
                last_emotion=wm.last_emotion,
                created_at=wm.created_at.isoformat(),
                updated_at=wm.updated_at.isoformat(),
            )
            for wm in memories
        ]
        return WorkingMemoryListResponse(memories=result, total=len(result))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–å·¥ä½œè®°å¿†åˆ—è¡¨å¤±è´¥: {str(e)}")


@app.get("/memory/working/{session_id}", response_model=WorkingMemoryInfo)
async def get_working_memory(session_id: str):
    """è·å–ä¼šè¯çš„å·¥ä½œè®°å¿†"""
    try:
        wm = memory_manager.get_working_memory(session_id)
        if not wm:
            raise HTTPException(status_code=404, detail="å·¥ä½œè®°å¿†ä¸å­˜åœ¨")
        return WorkingMemoryInfo(
            session_id=wm.session_id,
            current_topic=wm.current_topic,
            context_variables=wm.context_variables,
            turn_count=wm.turn_count,
            last_emotion=wm.last_emotion,
            created_at=wm.created_at.isoformat(),
            updated_at=wm.updated_at.isoformat(),
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–å·¥ä½œè®°å¿†å¤±è´¥: {str(e)}")


@app.delete("/memory/working/{session_id}", response_model=DeleteResponse)
async def delete_working_memory(session_id: str):
    """æ¸…é™¤æŒ‡å®šä¼šè¯çš„å·¥ä½œè®°å¿†"""
    try:
        success = memory_manager.delete_working_memory(session_id)
        if not success:
            raise HTTPException(status_code=404, detail="å·¥ä½œè®°å¿†ä¸å­˜åœ¨")
        return DeleteResponse(success=True, message="å·¥ä½œè®°å¿†å·²æ¸…é™¤")
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ¸…é™¤å·¥ä½œè®°å¿†å¤±è´¥: {str(e)}")


# ============ Memory Extract API ============

class MemoryExtractRequest(BaseModel):
    """æ™ºèƒ½æå–è®°å¿†è¯·æ±‚"""
    content: str
    role: str = "user"  # user | assistant


class MemoryExtractResponse(BaseModel):
    """æ™ºèƒ½æå–è®°å¿†å“åº”"""
    key: str
    value: str
    category: str
    confidence: float


@app.post("/memory/extract", response_model=MemoryExtractResponse)
async def extract_memory(request: MemoryExtractRequest):
    """ä»æ¶ˆæ¯å†…å®¹ä¸­æ™ºèƒ½æå–è®°å¿†ä¿¡æ¯

    ä½¿ç”¨ LLM åˆ†ææ¶ˆæ¯ï¼Œè‡ªåŠ¨æå–é”®åã€å€¼ã€ç±»åˆ«å’Œç½®ä¿¡åº¦ã€‚
    é€‚ç”¨äº"ä¿å­˜åˆ°é•¿æœŸè®°å¿†"å¯¹è¯æ¡†çš„é¢„å¡«å……ã€‚
    """
    try:
        # Try LLM extraction
        result = await memory_extractor.extract_memory_from_message(
            content=request.content,
            role=request.role,
        )

        if result and result.confidence >= 0.3:
            return MemoryExtractResponse(
                key=result.key,
                value=result.value,
                category=result.category,
                confidence=result.confidence,
            )

        # Fallback to default extraction
        default = memory_extractor.get_default_extraction(
            content=request.content,
            role=request.role,
        )
        return MemoryExtractResponse(
            key=default.key,
            value=default.value,
            category=default.category,
            confidence=default.confidence,
        )
    except Exception as e:
        # On error, return default extraction
        default = memory_extractor.get_default_extraction(
            content=request.content,
            role=request.role,
        )
        return MemoryExtractResponse(
            key=default.key,
            value=default.value,
            category=default.category,
            confidence=default.confidence,
        )


# ============ HITL API ============

class HITLRespondRequest(BaseModel):
    """HITL å“åº”è¯·æ±‚"""
    request_id: str
    session_id: str
    action: str  # approve | edit | reject
    data: Optional[Dict[str, Any]] = None


class HITLContinuationDataResponse(BaseModel):
    """HITL continuation data for frontend"""
    request_title: str
    action: str
    form_data: Optional[Dict[str, Any]] = None
    field_labels: Dict[str, str] = {}


class HITLRespondResponse(BaseModel):
    """HITL å“åº”ç»“æœ"""
    success: bool
    next_action: str = "continue"
    message: Optional[str] = None
    error: Optional[str] = None
    continuation_data: Optional[HITLContinuationDataResponse] = None


@app.post("/hitl/respond", response_model=HITLRespondResponse)
async def hitl_respond(request: HITLRespondRequest):
    """å¤„ç†ç”¨æˆ·å¯¹ HITL è¯·æ±‚çš„å“åº”

    ç”¨æˆ·å¯ä»¥é€‰æ‹© approveï¼ˆæ‰¹å‡†ï¼‰ã€editï¼ˆç¼–è¾‘åæäº¤ï¼‰æˆ– rejectï¼ˆæ‹’ç»/è·³è¿‡ï¼‰ã€‚
    """
    try:
        # è½¬æ¢ action å­—ç¬¦ä¸²ä¸ºæšä¸¾
        try:
            action = HITLAction(request.action)
        except ValueError:
            raise HTTPException(
                status_code=400,
                detail=f"æ— æ•ˆçš„ action: {request.action}ï¼Œå¿…é¡»æ˜¯ approve/edit/reject"
            )

        # æ„å»ºå“åº”æ•°æ®
        response_data = HITLResponseData(
            request_id=request.request_id,
            session_id=request.session_id,
            action=action,
            data=request.data,
        )

        # å¤„ç†å“åº”
        result = hitl_handler.process_hitl_response(response_data)

        # Convert continuation_data if present
        continuation_data_response = None
        if result.continuation_data:
            continuation_data_response = HITLContinuationDataResponse(
                request_title=result.continuation_data.request_title,
                action=result.continuation_data.action,
                form_data=result.continuation_data.form_data,
                field_labels=result.continuation_data.field_labels,
            )

        return HITLRespondResponse(
            success=result.success,
            next_action=result.next_action,
            message=result.message,
            error=result.error,
            continuation_data=continuation_data_response,
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¤„ç† HITL å“åº”å¤±è´¥: {str(e)}")


@app.get("/hitl/status/{request_id}")
async def hitl_status(request_id: str):
    """æ£€æŸ¥ HITL è¯·æ±‚çŠ¶æ€"""
    request_data = hitl_handler.get_hitl_request(request_id)
    if request_data is None:
        return {"exists": False, "expired": True}

    request, session_id, expires_at = request_data
    return {
        "exists": True,
        "expired": False,
        "session_id": session_id,
        "title": request.title,
        "expires_at": expires_at.isoformat(),
    }


class HITLContinueRequest(BaseModel):
    """HITL ç»§ç»­è¯·æ±‚"""
    session_id: str
    continuation_data: HITLContinuationDataResponse


async def stream_hitl_continuation(request: HITLContinueRequest):
    """æµå¼è°ƒç”¨ LLM API ç»§ç»­ HITL åçš„å¯¹è¯

    ä½¿ç”¨ continuation_data æ„å»ºä¸Šä¸‹æ–‡ï¼Œè°ƒç”¨ LLM ç»§ç»­å¯¹è¯ã€‚
    """
    import time
    start_time = time.time()
    print(f"[HITL-Continue] Starting continuation for session {request.session_id[:8]}...")

    try:
        config = load_chat_config()
    except FileNotFoundError as e:
        yield f'event: error\ndata: {json.dumps({"type": "error", "payload": {"message": str(e)}})}\n\n'
        return
    except Exception as e:
        yield f'event: error\ndata: {json.dumps({"type": "error", "payload": {"message": f"é…ç½®åŠ è½½å¤±è´¥: {str(e)}"}})}\n\n'
        return

    session_id = request.session_id

    # Build continuation context from the HITL response data
    from hitl_schema import HITLContinuationData
    continuation_data = HITLContinuationData(
        request_title=request.continuation_data.request_title,
        action=request.continuation_data.action,
        form_data=request.continuation_data.form_data,
        field_labels=request.continuation_data.field_labels,
    )

    context_start = time.time()
    hitl_context = hitl_handler.build_continuation_context(continuation_data)
    print(f"[HITL-Continue] Context built in {time.time() - context_start:.2f}s, length={len(hitl_context)}")

    # Build the continuation prompt
    prompt_start = time.time()
    system_prompt = chat_processor.build_hitl_continuation_prompt(session_id, hitl_context)
    print(f"[HITL-Continue] Prompt built in {time.time() - prompt_start:.2f}s, length={len(system_prompt)}")

    # æ·±åº¦æ€è€ƒå…³é—­æ—¶è¿½åŠ æç¤ºè¯
    if not is_deep_thinking_enabled():
        system_prompt += DISABLE_THINKING_PROMPT_SUFFIX

    # Prepare messages for LLM
    # Note: Many LLM APIs require at least one user message to respond properly
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": "è¯·æ ¹æ®æˆ‘åˆšæ‰æäº¤çš„è¡¨å•ä¿¡æ¯ç»™å‡ºå›å¤ã€‚"},
    ]
    print(f"[HITL-Continue] Total prompt length: {len(system_prompt)} chars, messages: {len(messages)}")

    # Call LLM API
    api_url = f"{config.api_base.rstrip('/')}/chat/completions"

    # ä½¿ç”¨ç»Ÿä¸€çš„è¯·æ±‚ä½“æ„å»ºå‡½æ•°ï¼ŒåŒ…å«æ·±åº¦æ€è€ƒå‚æ•°
    request_body = build_request_body_with_thinking(config, messages, stream=True)

    headers = {
        "Authorization": f"Bearer {config.api_key}",
        "Content-Type": "application/json"
    }

    assistant_response = ""
    detected_emotion = None

    try:
        print(f"[HITL-Continue] Sending request to {api_url}...")
        api_start = time.time()
        async with httpx.AsyncClient(timeout=60.0) as client:
            async with client.stream(
                "POST",
                api_url,
                json=request_body,
                headers=headers
            ) as response:
                print(f"[HITL-Continue] Got response status {response.status_code} in {time.time() - api_start:.2f}s")
                if response.status_code != 200:
                    error_text = await response.aread()
                    print(f"[HITL-Continue] API error: {error_text.decode()}")
                    yield f'event: error\ndata: {json.dumps({"type": "error", "payload": {"message": f"API è¯·æ±‚å¤±è´¥: {response.status_code} - {error_text.decode()}"}})}\n\n'
                    return

                first_chunk_time = None
                line_count = 0
                async for line in response.aiter_lines():
                    line_count += 1
                    # Debug: print first few lines
                    if line_count <= 3:
                        print(f"[HITL-Continue] Line {line_count}: {line[:100] if len(line) > 100 else line}")

                    if line.startswith("data: "):
                        if first_chunk_time is None:
                            first_chunk_time = time.time()
                            print(f"[HITL-Continue] First chunk received in {first_chunk_time - api_start:.2f}s")
                        data = line[6:]
                        if data.strip() == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data)
                            delta = chunk.get("choices", [{}])[0].get("delta", {})
                            content = delta.get("content", "")
                            if content:
                                assistant_response += content
                        except json.JSONDecodeError as e:
                            print(f"[HITL-Continue] JSON decode error: {e}, data: {data[:100] if len(data) > 100 else data}")
                            continue

                print(f"[HITL-Continue] Total lines received: {line_count}")

        print(f"[HITL-Continue] Response complete: length={len(assistant_response)}, total_time={time.time() - start_time:.2f}s")

        # Process the complete response
        # æ‰“å°å“åº”çŠ¶æ€æ—¥å¿—
        has_thinking_tags = "<thinking>" in assistant_response.lower() if assistant_response else False
        deep_thinking_mode = is_deep_thinking_enabled()
        print(f"[DeepThinking] HITL continuation response: length={len(assistant_response)}, has_thinking_tags={has_thinking_tags}, deep_thinking_enabled={deep_thinking_mode}")

        # åº”ç”¨ thinking æ ‡ç­¾è¿‡æ»¤ï¼ˆæ·±åº¦æ€è€ƒå…³é—­æ—¶ï¼‰
        final_response = chat_processor.filter_thinking_tags(assistant_response)

        # å¦‚æœè¿›è¡Œäº†è¿‡æ»¤ï¼Œæ‰“å°è¿‡æ»¤ç»“æœ
        if len(final_response) != len(assistant_response):
            print(f"[DeepThinking] Thinking tags filtered: original_length={len(assistant_response)}, filtered_length={len(final_response)}")

        hitl_request = None

        if assistant_response:
            try:
                # Parse LLM's JSON output
                print(f"[HITL-Continue] Processing LLM response: {assistant_response[:200]}...")
                chat_context = chat_processor.build_chat_context(session_id, hitl_context)
                chat_result = chat_processor.process_llm_response(assistant_response, chat_context)
                final_response = chat_result.response
                detected_emotion = chat_result.emotion
                print(f"[HITL-Continue] Parsed response: length={len(final_response)}, emotion={detected_emotion}")

                # Check for HITL request (chained HITL support)
                if chat_processor.is_hitl_enabled():
                    hitl_request = hitl_handler.extract_hitl_from_llm_response(assistant_response)
                    if hitl_request:
                        if isinstance(hitl_request, HITLDisplayRequest):
                            print(f"[HITL-Continue] Found visual_display request: id={hitl_request.id}")
                            hitl_handler.store_display_request(hitl_request, session_id)
                        else:
                            print(f"[HITL-Continue] Found form request: id={hitl_request.id}")
                            hitl_handler.store_hitl_request(hitl_request, session_id)

                # Send parsed response text
                print(f"[HITL-Continue] Sending text event: {final_response[:100] if len(final_response) > 100 else final_response}")
                yield f'event: text\ndata: {json.dumps({"type": "text", "payload": {"content": final_response}})}\n\n'

                # Send emotion info
                if detected_emotion:
                    yield f'event: emotion\ndata: {json.dumps({"type": "emotion", "payload": {"primary": detected_emotion.primary, "category": detected_emotion.category, "confidence": detected_emotion.confidence}})}\n\n'

                # Send HITL request event (for chained HITL)
                if hitl_request:
                    print(f"[HITL-Continue] Sending HITL SSE event for request {hitl_request.id}, type={hitl_request.type}")
                    if isinstance(hitl_request, HITLDisplayRequest):
                        hitl_payload = {
                            "id": hitl_request.id,
                            "type": hitl_request.type,
                            "title": hitl_request.title,
                            "description": hitl_request.description,
                            "displays": [
                                {
                                    "type": d.type.value if hasattr(d.type, 'value') else str(d.type),
                                    "data": d.data,
                                }
                                for d in hitl_request.displays
                            ],
                            "dismiss_label": hitl_request.dismiss_label,
                            "session_id": session_id,
                        }
                    else:
                        hitl_payload = {
                            "id": hitl_request.id,
                            "type": hitl_request.type,
                            "title": hitl_request.title,
                            "description": hitl_request.description,
                            "fields": [
                                {
                                    "name": f.name,
                                    "type": f.type.value,
                                    "label": f.label,
                                    "required": f.required,
                                    "placeholder": f.placeholder,
                                    "default": f.default,
                                    "options": [{"value": o.value, "label": o.label} for o in f.options] if f.options else None,
                                    "min": f.min,
                                    "max": f.max,
                                    "step": f.step,
                                }
                                for f in hitl_request.fields
                            ],
                            "actions": {
                                "approve": {"label": hitl_request.actions.approve.label, "style": hitl_request.actions.approve.style.value},
                                "edit": {"label": hitl_request.actions.edit.label, "style": hitl_request.actions.edit.style.value},
                                "reject": {"label": hitl_request.actions.reject.label, "style": hitl_request.actions.reject.style.value},
                            },
                            "session_id": session_id,
                        }
                    yield f'event: hitl\ndata: {json.dumps({"type": "hitl", "payload": hitl_payload})}\n\n'

                # æ£€æŸ¥æ˜¯å¦æœ‰ MCP å·¥å…·è°ƒç”¨è¯·æ±‚ (HITL continuation)
                tool_call_request = chat_processor.extract_tool_call(assistant_response)
                if tool_call_request:
                    print(f"[MCP] Found tool_call in continuation: name={tool_call_request.name}, method={tool_call_request.method}")
                    tool_result = await mcp_tool_executor.execute_mcp_tool(
                        service_name=tool_call_request.name,
                        method=tool_call_request.method,
                        arguments=tool_call_request.arguments,
                    )
                    tool_result_payload = {
                        "success": tool_result.success,
                        "tool_name": tool_result.tool_name,
                        "service_name": tool_result.service_name,
                        "result": tool_result.result,
                        "error": tool_result.error,
                    }
                    yield f'event: tool_result\ndata: {json.dumps({"type": "tool_result", "payload": tool_result_payload})}\n\n'
                    print(f"[MCP] Tool result sent: success={tool_result.success}")

                    # äºŒæ¬¡è°ƒç”¨ LLMï¼Œå°†å·¥å…·ç»“æœè½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€å›å¤
                    followup_response = await call_llm_with_tool_result(
                        config=config,
                        tool_result=tool_result,
                        user_context=hitl_context,
                    )
                    if followup_response:
                        # è§£æ followup å“åº”
                        try:
                            followup_parsed = parse_llm_output(followup_response)
                            followup_text = followup_parsed.response
                            print(f"[MCP Followup] Parsed response: {len(followup_text)} chars")
                        except Exception:
                            followup_text = chat_processor.extract_response_text(followup_response)
                            print(f"[MCP Followup] Using raw response: {len(followup_text)} chars")

                        # å‘é€ followup å“åº”
                        yield f'event: text\ndata: {json.dumps({"type": "text", "payload": {"content": followup_text}})}\n\n'
                        # æ›´æ–° final_response ç”¨äºä¿å­˜åˆ°æ•°æ®åº“
                        final_response = followup_text

            except Exception as e:
                print(f"è§£æ LLM continuation å“åº”å¤±è´¥: {e}")
                final_response = chat_processor.extract_response_text(assistant_response)
                yield f'event: text\ndata: {json.dumps({"type": "text", "payload": {"content": final_response}})}\n\n'

        # Save assistant message to database
        if final_response:
            try:
                chat_db.add_message(session_id, "assistant", final_response)
            except Exception as e:
                print(f"ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯å¤±è´¥: {e}")

        yield f'event: done\ndata: {json.dumps({"type": "done"})}\n\n'

    except httpx.TimeoutException:
        yield f'event: error\ndata: {json.dumps({"type": "error", "payload": {"message": "API è¯·æ±‚è¶…æ—¶"}})}\n\n'
    except httpx.ConnectError:
        yield f'event: error\ndata: {json.dumps({"type": "error", "payload": {"message": "æ— æ³•è¿æ¥åˆ° API æœåŠ¡å™¨"}})}\n\n'
    except Exception as e:
        yield f'event: error\ndata: {json.dumps({"type": "error", "payload": {"message": f"è¯·æ±‚é”™è¯¯: {str(e)}"}})}\n\n'


@app.post("/hitl/continue")
async def hitl_continue(request: HITLContinueRequest):
    """HITL ç»§ç»­å¯¹è¯æ¥å£ - è¿”å› SSE æµå¼å“åº”

    åœ¨ç”¨æˆ·å“åº” HITL è¡¨å•åï¼Œè‡ªåŠ¨è°ƒç”¨æ­¤æ¥å£ç»§ç»­å¯¹è¯ã€‚
    """
    return StreamingResponse(
        stream_hitl_continuation(request),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


# ============ Settings API ============

class SettingInfo(BaseModel):
    """è®¾ç½®é¡¹ä¿¡æ¯"""
    key: str
    value: Any
    value_type: str
    description: Optional[str] = None
    created_at: str
    updated_at: str


class SettingsListResponse(BaseModel):
    """è®¾ç½®åˆ—è¡¨å“åº”"""
    settings: Dict[str, Any]


class SettingsDetailListResponse(BaseModel):
    """è®¾ç½®è¯¦æƒ…åˆ—è¡¨å“åº”"""
    settings: List[SettingInfo]


class SettingUpdateRequest(BaseModel):
    """æ›´æ–°å•ä¸ªè®¾ç½®è¯·æ±‚"""
    value: Any
    value_type: Optional[str] = None


class SettingsBatchUpdateRequest(BaseModel):
    """æ‰¹é‡æ›´æ–°è®¾ç½®è¯·æ±‚"""
    settings: Dict[str, Any]


class SettingsBatchUpdateResponse(BaseModel):
    """æ‰¹é‡æ›´æ–°è®¾ç½®å“åº”"""
    success: bool
    updated_count: int


class SettingsResetResponse(BaseModel):
    """é‡ç½®è®¾ç½®å“åº”"""
    success: bool
    reset_count: int


@app.get("/api/settings", response_model=SettingsListResponse)
async def get_all_settings():
    """è·å–æ‰€æœ‰é…ç½®é¡¹"""
    try:
        settings = chat_db.get_all_settings()
        return SettingsListResponse(settings=settings)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–é…ç½®å¤±è´¥: {str(e)}")


@app.get("/api/settings/detail", response_model=SettingsDetailListResponse)
async def get_all_settings_detail():
    """è·å–æ‰€æœ‰é…ç½®é¡¹çš„è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬å…ƒæ•°æ®ï¼‰"""
    try:
        settings_list = chat_db.get_all_settings_with_metadata()
        result = [
            SettingInfo(
                key=s["key"],
                value=s["typed_value"],
                value_type=s["value_type"],
                description=s.get("description"),
                created_at=s["created_at"],
                updated_at=s["updated_at"],
            )
            for s in settings_list
        ]
        return SettingsDetailListResponse(settings=result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–é…ç½®è¯¦æƒ…å¤±è´¥: {str(e)}")


@app.get("/api/settings/{key:path}")
async def get_setting(key: str):
    """è·å–å•ä¸ªé…ç½®é¡¹"""
    try:
        value = chat_db.get_setting(key)
        if value is None:
            raise HTTPException(status_code=404, detail=f"é…ç½®é¡¹ '{key}' ä¸å­˜åœ¨")
        return {"key": key, "value": value}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–é…ç½®å¤±è´¥: {str(e)}")


@app.put("/api/settings/{key:path}")
async def update_setting(key: str, request: SettingUpdateRequest):
    """æ›´æ–°å•ä¸ªé…ç½®é¡¹"""
    try:
        success = chat_db.set_setting(key, request.value, request.value_type)
        if not success:
            raise HTTPException(status_code=500, detail="æ›´æ–°é…ç½®å¤±è´¥")
        return {"key": key, "value": request.value, "success": True}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°é…ç½®å¤±è´¥: {str(e)}")


@app.put("/api/settings", response_model=SettingsBatchUpdateResponse)
async def batch_update_settings(request: SettingsBatchUpdateRequest):
    """æ‰¹é‡æ›´æ–°é…ç½®é¡¹"""
    try:
        updated_count = chat_db.set_settings(request.settings)
        return SettingsBatchUpdateResponse(success=True, updated_count=updated_count)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡æ›´æ–°é…ç½®å¤±è´¥: {str(e)}")


@app.post("/api/settings/reset", response_model=SettingsResetResponse)
async def reset_settings():
    """é‡ç½®æ‰€æœ‰é…ç½®ä¸ºé»˜è®¤å€¼"""
    try:
        reset_count = chat_db.reset_settings()
        return SettingsResetResponse(success=True, reset_count=reset_count)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é‡ç½®é…ç½®å¤±è´¥: {str(e)}")


# ============ Plans API ============

class PlanInfo(BaseModel):
    """è®¡åˆ’ä¿¡æ¯"""
    id: str
    session_id: Optional[str] = None
    title: str
    description: Optional[str] = None
    target_time: str
    reminder_offset_minutes: int
    repeat_type: str
    status: str
    snooze_until: Optional[str] = None
    created_at: str
    updated_at: str


class PlanCreateRequest(BaseModel):
    """åˆ›å»ºè®¡åˆ’è¯·æ±‚"""
    title: str
    description: Optional[str] = None
    target_time: str  # ISO format datetime
    reminder_offset_minutes: int = 10
    repeat_type: str = "none"
    session_id: Optional[str] = None


class PlanUpdateRequest(BaseModel):
    """æ›´æ–°è®¡åˆ’è¯·æ±‚"""
    title: Optional[str] = None
    description: Optional[str] = None
    target_time: Optional[str] = None
    reminder_offset_minutes: Optional[int] = None
    repeat_type: Optional[str] = None
    status: Optional[str] = None


class PlanListResponse(BaseModel):
    """è®¡åˆ’åˆ—è¡¨å“åº”"""
    plans: List[PlanInfo]
    total: int


class PlanSnoozeRequest(BaseModel):
    """æ¨è¿Ÿè®¡åˆ’è¯·æ±‚"""
    snooze_minutes: int = 10


def _plan_entry_to_info(entry: memory_manager.PlanEntry) -> PlanInfo:
    """Convert PlanEntry to PlanInfo."""
    return PlanInfo(
        id=entry.id,
        session_id=entry.session_id,
        title=entry.title,
        description=entry.description,
        target_time=entry.target_time.isoformat(),
        reminder_offset_minutes=entry.reminder_offset_minutes,
        repeat_type=entry.repeat_type,
        status=entry.status,
        snooze_until=entry.snooze_until.isoformat() if entry.snooze_until else None,
        created_at=entry.created_at.isoformat(),
        updated_at=entry.updated_at.isoformat(),
    )


@app.get("/plans", response_model=PlanListResponse)
async def list_plans(
    status: Optional[str] = None,
    limit: int = 100,
    offset: int = 0,
):
    """è·å–è®¡åˆ’åˆ—è¡¨

    æ”¯æŒæŒ‰çŠ¶æ€ç­›é€‰å’Œåˆ†é¡µã€‚
    """
    try:
        entries = memory_manager.list_plans(
            status=status,
            limit=limit,
            offset=offset,
        )
        total = memory_manager.count_plans(status=status)

        plans = [_plan_entry_to_info(e) for e in entries]
        return PlanListResponse(plans=plans, total=total)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–è®¡åˆ’åˆ—è¡¨å¤±è´¥: {str(e)}")


@app.get("/plans/due", response_model=PlanListResponse)
async def get_due_plans(limit: int = 10):
    """è·å–åˆ°æœŸçš„è®¡åˆ’

    è¿”å›æ‰€æœ‰ status ä¸º pending ä¸”æé†’æ—¶é—´å·²åˆ°çš„è®¡åˆ’ã€‚
    ç”¨äº Electron è½®è¯¢ã€‚
    """
    try:
        entries = memory_manager.get_due_plans(limit=limit)
        plans = [_plan_entry_to_info(e) for e in entries]
        return PlanListResponse(plans=plans, total=len(plans))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–åˆ°æœŸè®¡åˆ’å¤±è´¥: {str(e)}")


@app.post("/plans", response_model=PlanInfo)
async def create_plan(request: PlanCreateRequest):
    """åˆ›å»ºæ–°è®¡åˆ’"""
    try:
        target_time = datetime.fromisoformat(request.target_time.replace("Z", "+00:00"))
        entry = memory_manager.create_plan(
            title=request.title,
            description=request.description,
            target_time=target_time,
            session_id=request.session_id,
            reminder_offset_minutes=request.reminder_offset_minutes,
            repeat_type=request.repeat_type,
        )
        return _plan_entry_to_info(entry)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=f"æ—¶é—´æ ¼å¼é”™è¯¯: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"åˆ›å»ºè®¡åˆ’å¤±è´¥: {str(e)}")


@app.get("/plans/{plan_id}", response_model=PlanInfo)
async def get_plan(plan_id: str):
    """è·å–ç‰¹å®šè®¡åˆ’è¯¦æƒ…"""
    try:
        entry = memory_manager.get_plan(plan_id)
        if not entry:
            raise HTTPException(status_code=404, detail="è®¡åˆ’ä¸å­˜åœ¨")
        return _plan_entry_to_info(entry)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–è®¡åˆ’å¤±è´¥: {str(e)}")


@app.put("/plans/{plan_id}", response_model=PlanInfo)
async def update_plan(plan_id: str, request: PlanUpdateRequest):
    """æ›´æ–°è®¡åˆ’"""
    try:
        target_time = None
        if request.target_time:
            target_time = datetime.fromisoformat(request.target_time.replace("Z", "+00:00"))

        entry = memory_manager.update_plan(
            plan_id=plan_id,
            title=request.title,
            description=request.description,
            target_time=target_time,
            reminder_offset_minutes=request.reminder_offset_minutes,
            repeat_type=request.repeat_type,
            status=request.status,
        )
        if not entry:
            raise HTTPException(status_code=404, detail="è®¡åˆ’ä¸å­˜åœ¨")
        return _plan_entry_to_info(entry)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"æ›´æ–°è®¡åˆ’å¤±è´¥: {str(e)}")


@app.delete("/plans/{plan_id}", response_model=DeleteResponse)
async def delete_plan(plan_id: str):
    """åˆ é™¤è®¡åˆ’"""
    try:
        success = memory_manager.delete_plan(plan_id)
        if not success:
            raise HTTPException(status_code=404, detail="è®¡åˆ’ä¸å­˜åœ¨")
        return DeleteResponse(success=True, message="è®¡åˆ’åˆ é™¤æˆåŠŸ")
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤è®¡åˆ’å¤±è´¥: {str(e)}")


@app.post("/plans/{plan_id}/complete", response_model=PlanInfo)
async def complete_plan(plan_id: str):
    """å®Œæˆè®¡åˆ’

    å¦‚æœæ˜¯é‡å¤è®¡åˆ’ï¼Œä¼šè‡ªåŠ¨åˆ›å»ºä¸‹ä¸€ä¸ªå‘¨æœŸçš„è®¡åˆ’ã€‚
    """
    try:
        entry = memory_manager.complete_plan(plan_id)
        if not entry:
            raise HTTPException(status_code=404, detail="è®¡åˆ’ä¸å­˜åœ¨")
        return _plan_entry_to_info(entry)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å®Œæˆè®¡åˆ’å¤±è´¥: {str(e)}")


@app.post("/plans/{plan_id}/dismiss", response_model=PlanInfo)
async def dismiss_plan(plan_id: str):
    """å–æ¶ˆè®¡åˆ’"""
    try:
        entry = memory_manager.dismiss_plan(plan_id)
        if not entry:
            raise HTTPException(status_code=404, detail="è®¡åˆ’ä¸å­˜åœ¨")
        return _plan_entry_to_info(entry)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å–æ¶ˆè®¡åˆ’å¤±è´¥: {str(e)}")


@app.post("/plans/{plan_id}/snooze", response_model=PlanInfo)
async def snooze_plan(plan_id: str, request: PlanSnoozeRequest):
    """æ¨è¿Ÿè®¡åˆ’æé†’"""
    try:
        entry = memory_manager.snooze_plan(plan_id, request.snooze_minutes)
        if not entry:
            raise HTTPException(status_code=404, detail="è®¡åˆ’ä¸å­˜åœ¨")
        return _plan_entry_to_info(entry)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ¨è¿Ÿè®¡åˆ’å¤±è´¥: {str(e)}")


# ============ MCP Service Management API ============

class MCPServerCreateRequest(BaseModel):
    """åˆ›å»º MCP æœåŠ¡è¯·æ±‚"""
    name: str
    command: str
    args: List[str] = []
    env: Dict[str, str] = {}
    enabled: bool = True
    auto_start: bool = False
    description: Optional[str] = None
    trigger_keywords: List[str] = []


class MCPServerUpdateRequest(BaseModel):
    """æ›´æ–° MCP æœåŠ¡è¯·æ±‚"""
    name: Optional[str] = None
    command: Optional[str] = None
    args: Optional[List[str]] = None
    env: Optional[Dict[str, str]] = None
    enabled: Optional[bool] = None
    auto_start: Optional[bool] = None
    description: Optional[str] = None
    trigger_keywords: Optional[List[str]] = None


class MCPServerInfoResponse(BaseModel):
    """MCP æœåŠ¡ä¿¡æ¯å“åº”"""
    id: str
    name: str
    command: str
    args: List[str]
    env: Dict[str, str]
    enabled: bool
    auto_start: bool = False
    created_at: str
    status: str
    error_message: Optional[str] = None
    pid: Optional[int] = None
    description: Optional[str] = None
    trigger_keywords: List[str] = []


class MCPServerListResponse(BaseModel):
    """MCP æœåŠ¡åˆ—è¡¨å“åº”"""
    servers: List[MCPServerInfoResponse]
    total: int


class MCPServerActionResponse(BaseModel):
    """MCP æœåŠ¡æ“ä½œå“åº”"""
    success: bool
    message: Optional[str] = None
    server: Optional[MCPServerInfoResponse] = None


def _mcp_server_to_response(info: mcp_manager.MCPServerInfo) -> MCPServerInfoResponse:
    """Convert MCPServerInfo to response model."""
    return MCPServerInfoResponse(
        id=info.id,
        name=info.name,
        command=info.command,
        args=info.args,
        env=info.env,
        enabled=info.enabled,
        auto_start=info.auto_start,
        created_at=info.created_at,
        status=info.status.value if hasattr(info.status, 'value') else str(info.status),
        error_message=info.error_message,
        pid=info.pid,
        description=info.description,
        trigger_keywords=info.trigger_keywords,
    )


@app.get("/api/mcp/servers", response_model=MCPServerListResponse)
async def list_mcp_servers():
    """è·å–æ‰€æœ‰ MCP æœåŠ¡åˆ—è¡¨åŠçŠ¶æ€"""
    try:
        print("[MCP API] GET /api/mcp/servers - listing all servers")
        pm = mcp_manager.get_process_manager()
        servers = pm.list_servers_with_status()
        print(f"[MCP API] Listed {len(servers)} servers")
        return MCPServerListResponse(
            servers=[_mcp_server_to_response(s) for s in servers],
            total=len(servers)
        )
    except Exception as e:
        print(f"[MCP API] Error listing servers: {e}")
        raise HTTPException(status_code=500, detail=f"è·å– MCP æœåŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}")


@app.post("/api/mcp/servers", response_model=MCPServerInfoResponse, status_code=201)
async def create_mcp_server(request: MCPServerCreateRequest):
    """æ³¨å†Œæ–°çš„ MCP æœåŠ¡"""
    try:
        print(f"[MCP API] POST /api/mcp/servers - creating server: name={request.name}, command={request.command}")
        registry = mcp_manager.get_registry()
        config = mcp_manager.MCPServerConfig(
            name=request.name,
            command=request.command,
            args=request.args,
            env=request.env,
            enabled=request.enabled,
            auto_start=request.auto_start,
            description=request.description,
            trigger_keywords=request.trigger_keywords,
        )
        created = registry.add_server(config)
        print(f"[MCP API] Server created: id={created.id}, name={created.name}")

        # Get info with status
        pm = mcp_manager.get_process_manager()
        info = pm.get_server_info(created.id)
        if info:
            return _mcp_server_to_response(info)

        raise HTTPException(status_code=500, detail="æœåŠ¡åˆ›å»ºæˆåŠŸä½†æ— æ³•è·å–çŠ¶æ€")
    except ValueError as e:
        print(f"[MCP API] Create server failed: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        print(f"[MCP API] Create server error: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ›å»º MCP æœåŠ¡å¤±è´¥: {str(e)}")


@app.get("/api/mcp/servers/{server_id}", response_model=MCPServerInfoResponse)
async def get_mcp_server(server_id: str):
    """è·å–å•ä¸ª MCP æœåŠ¡è¯¦æƒ…"""
    try:
        print(f"[MCP API] GET /api/mcp/servers/{server_id}")
        pm = mcp_manager.get_process_manager()
        info = pm.get_server_info(server_id)
        if not info:
            print(f"[MCP API] Server not found: id={server_id}")
            raise HTTPException(status_code=404, detail="MCP æœåŠ¡ä¸å­˜åœ¨")
        print(f"[MCP API] Found server: name={info.name}, status={info.status}")
        return _mcp_server_to_response(info)
    except HTTPException:
        raise
    except Exception as e:
        print(f"[MCP API] Get server error: {e}")
        raise HTTPException(status_code=500, detail=f"è·å– MCP æœåŠ¡å¤±è´¥: {str(e)}")


@app.put("/api/mcp/servers/{server_id}", response_model=MCPServerInfoResponse)
async def update_mcp_server(server_id: str, request: MCPServerUpdateRequest):
    """æ›´æ–° MCP æœåŠ¡é…ç½®"""
    try:
        print(f"[MCP API] PUT /api/mcp/servers/{server_id} - updating")
        pm = mcp_manager.get_process_manager()

        # Check if running - warn user
        status = pm.get_status(server_id)
        if status == mcp_manager.MCPServerStatus.RUNNING:
            print(f"[MCP API] Update blocked: server {server_id} is running")
            raise HTTPException(
                status_code=400,
                detail="æœåŠ¡æ­£åœ¨è¿è¡Œï¼Œè¯·å…ˆåœæ­¢æœåŠ¡å†æ›´æ–°é…ç½®"
            )

        registry = mcp_manager.get_registry()
        updated = registry.update_server(
            server_id,
            name=request.name,
            command=request.command,
            args=request.args,
            env=request.env,
            enabled=request.enabled,
            auto_start=request.auto_start,
            description=request.description,
            trigger_keywords=request.trigger_keywords,
        )
        if not updated:
            print(f"[MCP API] Update failed: server not found id={server_id}")
            raise HTTPException(status_code=404, detail="MCP æœåŠ¡ä¸å­˜åœ¨")

        info = pm.get_server_info(server_id)
        if info:
            print(f"[MCP API] Server updated: name={info.name}")
            return _mcp_server_to_response(info)

        raise HTTPException(status_code=500, detail="æ›´æ–°æˆåŠŸä½†æ— æ³•è·å–çŠ¶æ€")
    except HTTPException:
        raise
    except ValueError as e:
        print(f"[MCP API] Update validation error: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        print(f"[MCP API] Update error: {e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–° MCP æœåŠ¡å¤±è´¥: {str(e)}")


@app.delete("/api/mcp/servers/{server_id}", status_code=204)
async def delete_mcp_server(server_id: str):
    """åˆ é™¤ MCP æœåŠ¡"""
    try:
        print(f"[MCP API] DELETE /api/mcp/servers/{server_id}")
        pm = mcp_manager.get_process_manager()
        registry = mcp_manager.get_registry()

        # Stop if running
        print(f"[MCP API] Stopping server before delete: id={server_id}")
        pm.stop_server(server_id)

        # Delete config
        success = registry.delete_server(server_id)
        if not success:
            print(f"[MCP API] Delete failed: server not found id={server_id}")
            raise HTTPException(status_code=404, detail="MCP æœåŠ¡ä¸å­˜åœ¨")

        print(f"[MCP API] Server deleted: id={server_id}")
        return None
    except HTTPException:
        raise
    except Exception as e:
        print(f"[MCP API] Delete error: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤ MCP æœåŠ¡å¤±è´¥: {str(e)}")


@app.post("/api/mcp/servers/{server_id}/start", response_model=MCPServerActionResponse)
async def start_mcp_server(server_id: str):
    """å¯åŠ¨ MCP æœåŠ¡"""
    try:
        print(f"[MCP API] POST /api/mcp/servers/{server_id}/start")
        pm = mcp_manager.get_process_manager()

        # Check if server exists
        info = pm.get_server_info(server_id)
        if not info:
            print(f"[MCP API] Start failed: server not found id={server_id}")
            raise HTTPException(status_code=404, detail="MCP æœåŠ¡ä¸å­˜åœ¨")

        print(f"[MCP API] Starting server: name={info.name}")
        success = pm.start_server(server_id)

        # Get updated info
        info = pm.get_server_info(server_id)
        if info:
            print(f"[MCP API] Start result: success={success}, status={info.status}, pid={info.pid}")

            # If server started successfully, fetch its tools list for caching
            if success and info.status == mcp_manager.MCPServerStatus.RUNNING:
                try:
                    print(f"[MCP API] Fetching tools list for newly started server: {info.name}")
                    import asyncio
                    # Give the server a moment to initialize
                    await asyncio.sleep(0.5)
                    tools = await mcp_tool_executor.list_service_tools(info.name)
                    print(f"[MCP API] Cached {len(tools)} tools from server: {info.name}")
                except Exception as e:
                    print(f"[MCP API] Failed to fetch tools list: {e}")

            return MCPServerActionResponse(
                success=success,
                message="æœåŠ¡å¯åŠ¨æˆåŠŸ" if success else info.error_message,
                server=_mcp_server_to_response(info),
            )

        return MCPServerActionResponse(
            success=success,
            message="æœåŠ¡å¯åŠ¨æˆåŠŸ" if success else "å¯åŠ¨å¤±è´¥",
        )
    except HTTPException:
        raise
    except Exception as e:
        print(f"[MCP API] Start error: {e}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨ MCP æœåŠ¡å¤±è´¥: {str(e)}")


@app.post("/api/mcp/servers/{server_id}/stop", response_model=MCPServerActionResponse)
async def stop_mcp_server(server_id: str):
    """åœæ­¢ MCP æœåŠ¡"""
    try:
        print(f"[MCP API] POST /api/mcp/servers/{server_id}/stop")
        pm = mcp_manager.get_process_manager()

        # Check if server exists
        info = pm.get_server_info(server_id)
        if not info:
            print(f"[MCP API] Stop failed: server not found id={server_id}")
            raise HTTPException(status_code=404, detail="MCP æœåŠ¡ä¸å­˜åœ¨")

        print(f"[MCP API] Stopping server: name={info.name}, pid={info.pid}")
        success = pm.stop_server(server_id)

        # Get updated info
        info = pm.get_server_info(server_id)
        if info:
            print(f"[MCP API] Stop result: success={success}, status={info.status}")
            return MCPServerActionResponse(
                success=success,
                message="æœåŠ¡å·²åœæ­¢" if success else info.error_message,
                server=_mcp_server_to_response(info),
            )

        return MCPServerActionResponse(
            success=success,
            message="æœåŠ¡å·²åœæ­¢" if success else "åœæ­¢å¤±è´¥",
        )
    except HTTPException:
        raise
    except Exception as e:
        print(f"[MCP API] Stop error: {e}")
        raise HTTPException(status_code=500, detail=f"åœæ­¢ MCP æœåŠ¡å¤±è´¥: {str(e)}")


@app.post("/api/mcp/servers/{server_id}/restart", response_model=MCPServerActionResponse)
async def restart_mcp_server(server_id: str):
    """é‡å¯ MCP æœåŠ¡"""
    try:
        print(f"[MCP API] POST /api/mcp/servers/{server_id}/restart")
        pm = mcp_manager.get_process_manager()

        # Check if server exists
        info = pm.get_server_info(server_id)
        if not info:
            print(f"[MCP API] Restart failed: server not found id={server_id}")
            raise HTTPException(status_code=404, detail="MCP æœåŠ¡ä¸å­˜åœ¨")

        print(f"[MCP API] Restarting server: name={info.name}")
        success = pm.restart_server(server_id)

        # Get updated info
        info = pm.get_server_info(server_id)
        if info:
            print(f"[MCP API] Restart result: success={success}, status={info.status}, pid={info.pid}")

            # If server restarted successfully, fetch its tools list for caching
            if success and info.status == mcp_manager.MCPServerStatus.RUNNING:
                try:
                    print(f"[MCP API] Fetching tools list for restarted server: {info.name}")
                    import asyncio
                    # Give the server a moment to initialize
                    await asyncio.sleep(0.5)
                    # Clear old cache and fetch new tools
                    mcp_tool_executor.get_executor().clear_tools_cache(info.name)
                    tools = await mcp_tool_executor.list_service_tools(info.name)
                    print(f"[MCP API] Cached {len(tools)} tools from server: {info.name}")
                except Exception as e:
                    print(f"[MCP API] Failed to fetch tools list: {e}")

            return MCPServerActionResponse(
                success=success,
                message="æœåŠ¡é‡å¯æˆåŠŸ" if success else info.error_message,
                server=_mcp_server_to_response(info),
            )

        return MCPServerActionResponse(
            success=success,
            message="æœåŠ¡é‡å¯æˆåŠŸ" if success else "é‡å¯å¤±è´¥",
        )
    except HTTPException:
        raise
    except Exception as e:
        print(f"[MCP API] Restart error: {e}")
        raise HTTPException(status_code=500, detail=f"é‡å¯ MCP æœåŠ¡å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8002,
        reload=True
    )